{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install better_profanity (geen grap, aub. installeren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import unified_diff\n",
    "\n",
    "def make_diff(old, new):\n",
    "    return '\\n'.join([ l for l in unified_diff(old.split('\\n'), new.split('\\n')) if l.startswith('+') or l.startswith('-') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from better_profanity import profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(file_name, content):\n",
    "    with open(file_name,'a', encoding=\"utf-8\") as fd:\n",
    "        wr = csv.writer(fd, dialect='excel')\n",
    "        wr.writerow(content)\n",
    "        \n",
    "def calc_dels_adds(diff):\n",
    "    deletions = 0\n",
    "    additions = 0\n",
    "    for line in diff.split('\\n'):\n",
    "        if len(line)> 0:\n",
    "            if line[0] == \"-\": deletions += 1\n",
    "            if line[0] == \"+\": additions += 1\n",
    "    del_add_ratio = deletions / additions\n",
    "    return int(deletions), int(additions), del_add_ratio\n",
    "\n",
    "def count_profanity(diff):\n",
    "    nr_words, nr_profanities = 0, 0\n",
    "    for word in diff.split(' '):\n",
    "        nr_words += 1\n",
    "        if profanity.contains_profanity(word): \n",
    "            nr_profanities += 1 \n",
    "    return nr_profanities / nr_words\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted 126478 part-files in 189803 dirs, with last one being <DirEntry 'outputs-1587648840000'>\n"
     ]
    }
   ],
   "source": [
    "# test until when I created 'part'-files, was 3 April 7:36\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "PATH = r\"C:\\Users\\Hans\\PhD\\Course_Data_Analytics\\Assignment_3\\streaming\"\n",
    "\n",
    "try: \n",
    "    os.remove(FILE_NAME)\n",
    "except: \n",
    "     pass   \n",
    "\n",
    "counter_part = 0\n",
    "counter_dir = 0\n",
    "last_dir = \"\"\n",
    "with os.scandir(PATH) as streaming_directory:\n",
    "    for entry in streaming_directory:\n",
    "        with os.scandir(entry) as output_directory:\n",
    "            for entry_sub in output_directory:\n",
    "                if entry_sub.name.startswith(\"part\") and entry_sub.is_file():\n",
    "                    counter_part += 1\n",
    "                    last_dir = entry\n",
    "        counter_dir += 1\n",
    "print(\"counted {} part-files in {} dirs, with last one being {}\".format(counter_part, counter_dir, last_dir))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "PATH = r\"C:\\Users\\Hans\\PhD\\Course_Data_Analytics\\Assignment_3\\streaming\"\n",
    "FILE_NAME = \"Wikipedia.csv\"\n",
    "NR_FILES = 10000\n",
    "\n",
    "try: \n",
    "    os.remove(FILE_NAME)\n",
    "except: \n",
    "     pass   \n",
    "\n",
    "counter = 0\n",
    "columns =['title_page', 'name_user', 'label', 'comment', 'url_page', 'deletions', 'additions', 'del_add_ratio', 'profanity']\n",
    "write_csv(FILE_NAME, columns)  # create header in csv file\n",
    "with os.scandir(PATH) as streaming_directory:\n",
    "    for entry in streaming_directory:\n",
    "        with os.scandir(entry) as output_directory:\n",
    "            if counter > NR_FILES: break\n",
    "            for entry_sub in output_directory:\n",
    "                if entry_sub.name.startswith(\"part\") and entry_sub.is_file():\n",
    "                    if counter > NR_FILES - 1: break\n",
    "                    #print(entry_sub.name, entry_sub.path)\n",
    "                    counter += 1\n",
    "                    with open(entry_sub.path, \"r\") as file:\n",
    "                        try:\n",
    "                            text = file.read()\n",
    "                            text_dict = json.loads(text)\n",
    "                            diff = make_diff(text_dict[\"text_old\"], text_dict[\"text_new\"])\n",
    "                            deletions, additions, del_add_ratio = calc_dels_adds(diff)\n",
    "                            text_dict[\"deletions\"] = deletions\n",
    "                            text_dict[\"additions\"] = additions\n",
    "                            text_dict[\"del_add_ratio\"] = del_add_ratio\n",
    "                            text_dict[\"profanity\"] = count_profanity(diff)\n",
    "                            myCsvRow = [text_dict[col] for col in columns]\n",
    "                            write_csv(FILE_NAME, myCsvRow)\n",
    "                        except:\n",
    "                            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as funcs\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    ".master(\"local[4]\")\\\n",
    ".appName(\"ReadFromCsv\")\\\n",
    ".config(\"spark.driver.memory\",\"3g\")\\\n",
    ".config(\"spark.executor.memory\", \"4g\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logger = spark.sparkContext._jvm.org.apache.log4j\\nlogger.LogManager.getLogger(\"org\"). setLevel(logger.Level.ERROR)\\nlogger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''logger = spark.sparkContext._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel(logger.Level.ERROR)\n",
    "logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerische data worden soms as string ingelezen. Moet daarom input format definieren\n",
    "from pyspark.sql.types import StructField,IntegerType, DoubleType, StringType\n",
    "newDF=[StructField('title_page',StringType(),True),\n",
    "       StructField('name_user',StringType(),True),\n",
    "       StructField('label',StringType(),True),\n",
    "       StructField('comment',StringType(),True),\n",
    "       StructField('url_page',StringType(),True),\n",
    "       StructField('deletions' ,IntegerType(),True),\n",
    "       StructField('additions' ,IntegerType(),True),\n",
    "       StructField('del_add_ratio',DoubleType(),True),\n",
    "       StructField('profanity',DoubleType(),True)\n",
    "       ]\n",
    "finalStruct=StructType(fields=newDF)\n",
    "#df=spark.read.csv('ctor.csv',schema=finalStruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train = spark.read \\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".schema(finalStruct)\\\n",
    ".load(\"Wikipedia.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      " |-- deletions: integer (nullable = true)\n",
      " |-- additions: integer (nullable = true)\n",
      " |-- del_add_ratio: double (nullable = true)\n",
      " |-- profanity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de inputs voor het model \n",
    "feature_cols = ['deletions', 'additions', 'del_add_ratio', 'profanity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ik denk dat hier de labels in strings omgevormd worden, en die kolom 'target' genoemd wordt\n",
    "label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = feature_cols, outputCol = 'features',  handleInvalid =  \"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[assembler, label_indexer])\n",
    "pipe_model = pipe.fit(wiki_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pipe_model.transform(wiki_train)\n",
    "data = data.select(\"features\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               features  target\n",
      "0  [1.0, 2.0, 0.5, 0.0]     0.0\n",
      "1  [1.0, 2.0, 0.5, 0.0]     0.0\n",
      "2  [1.0, 2.0, 0.5, 0.0]     0.0\n",
      "3  [1.0, 2.0, 0.5, 0.0]     0.0\n",
      "4  [1.0, 2.0, 0.5, 0.0]     0.0\n"
     ]
    }
   ],
   "source": [
    "print(train.toPandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Algorithm\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>target</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0, 2.0, 0.5, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1769.0, 88.0, 7.0]</td>\n",
       "      <td>[0.9490343347639485, 0.04721030042918455, 0.00...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.0, 2.0, 0.5, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1769.0, 88.0, 7.0]</td>\n",
       "      <td>[0.9490343347639485, 0.04721030042918455, 0.00...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0, 2.0, 0.5, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1769.0, 88.0, 7.0]</td>\n",
       "      <td>[0.9490343347639485, 0.04721030042918455, 0.00...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 2.0, 0.5, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1769.0, 88.0, 7.0]</td>\n",
       "      <td>[0.9490343347639485, 0.04721030042918455, 0.00...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.0, 2.0, 0.5, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1769.0, 88.0, 7.0]</td>\n",
       "      <td>[0.9490343347639485, 0.04721030042918455, 0.00...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               features  target        rawPrediction  \\\n",
       "0  [1.0, 2.0, 0.5, 0.0]     0.0  [1769.0, 88.0, 7.0]   \n",
       "1  [1.0, 2.0, 0.5, 0.0]     0.0  [1769.0, 88.0, 7.0]   \n",
       "2  [1.0, 2.0, 0.5, 0.0]     0.0  [1769.0, 88.0, 7.0]   \n",
       "3  [1.0, 2.0, 0.5, 0.0]     0.0  [1769.0, 88.0, 7.0]   \n",
       "4  [1.0, 2.0, 0.5, 0.0]     0.0  [1769.0, 88.0, 7.0]   \n",
       "\n",
       "                                         probability  prediction  \n",
       "0  [0.9490343347639485, 0.04721030042918455, 0.00...         0.0  \n",
       "1  [0.9490343347639485, 0.04721030042918455, 0.00...         0.0  \n",
       "2  [0.9490343347639485, 0.04721030042918455, 0.00...         0.0  \n",
       "3  [0.9490343347639485, 0.04721030042918455, 0.00...         0.0  \n",
       "4  [0.9490343347639485, 0.04721030042918455, 0.00...         0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and Predicting of Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"target\", featuresCol=\"features\")\n",
    "modeldt = dt.fit(train)\n",
    "predictiondt = modeldt.transform(test)\n",
    "predictiondt.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>count</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2525</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>301</td>\n",
       "      <td>unsafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>vandal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>unsafe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction  target  count  status\n",
       "0         0.0     0.0   2525    safe\n",
       "1         0.0     1.0    301  unsafe\n",
       "2         0.0     2.0     20  vandal\n",
       "3         1.0     0.0     11    safe\n",
       "4         1.0     1.0     20  unsafe"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix of Decision Tree\n",
    "# 0 is safe\n",
    "# 1 is unsafe\n",
    "# 2 (other) is vandal\n",
    "predictiondt.select(\"prediction\", \"target\")\\\n",
    ".groupBy(\"prediction\", \"target\").count()\\\n",
    ".orderBy(\"prediction\", \"target\", ascending=True).withColumn(\"status\",\n",
    "funcs.when(funcs.col(\"target\").isin(0), \"safe\")\\\n",
    "    .when(funcs.col(\"target\").isin(1), \"unsafe\") \\\n",
    ".otherwise(\"vandal\")).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  prediction  count\n",
       "0     2.0         0.0     20\n",
       "1     1.0         1.0     20\n",
       "2     0.0         1.0     11\n",
       "3     1.0         0.0    301\n",
       "4     2.0         2.0      2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictiondt.groupBy([\"target\",\"prediction\"]).count().toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree --- \n",
      "Accuracy Rate = 0.8835\n",
      "  Error  Rate = 0.1165 \n"
     ]
    }
   ],
   "source": [
    "# Calculation of Accuracy  (is natuurlijk niet de meest geschikte metric voor dit probleem)\n",
    "evaluatordt = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt = evaluatordt.evaluate(predictiondt)\n",
    "\n",
    "print(\"--- Decision Tree --- \")\n",
    "print(\"Accuracy Rate =\", round(dt,4))\n",
    "print(\"  Error  Rate = %g \" % round((1.0 - dt),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|  safe|unsage|vandal|\n",
      "+------+------+------+\n",
      "|2525.0|  11.0|   0.0|\n",
      "| 301.0|  20.0|   4.0|\n",
      "|  20.0|   0.0|   2.0|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vandal ontbreekt nog, was nog niet in training dataset\n",
    "predictionAndLabel = predictiondt.select(\"prediction\", \"target\").rdd\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics = MulticlassMetrics(predictionAndLabel)\n",
    "cm = metrics.confusionMatrix()\n",
    "rows = cm.toArray().tolist()\n",
    "\n",
    "confusion_matrix = spark.createDataFrame(rows,[\"safe\",\"unsage\", \"vandal\"])\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---+---+\n",
      "|prediction_target| 0.0|1.0|2.0|\n",
      "+-----------------+----+---+---+\n",
      "|              2.0|   0|  4|  2|\n",
      "|              1.0|  11| 20|  0|\n",
      "|              0.0|2525|301| 20|\n",
      "+-----------------+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0 is safe\n",
    "# 1 is unsafe\n",
    "# 2 (other) is vandal\n",
    "predictiondt.withColumn(\"A\", funcs.struct(\"prediction\",\"target\")).crosstab(\"prediction\",\"target\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STREAMING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "[\n",
    "    StructField(\"title_page\", StringType(), True),\n",
    "    StructField(\"name_user\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"url_page\", StringType(), True),\n",
    "    StructField(\"deletions\", StringType(), True),\n",
    "    StructField(\"additions\", IntegerType(), True),\n",
    "    StructField(\"del_add_ratio\", DoubleType(), True),\n",
    "    StructField(\"profanity\", DoubleType(), True),\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Streaming Data\n",
    "#ik gebruik hier voorlopig de training set, moet later de stream worden\n",
    "wiki_test = spark.readStream \\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".schema(schema)\\\n",
    ".load(\"Wikipedia.csv\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      " |-- deletions: string (nullable = true)\n",
      " |-- additions: integer (nullable = true)\n",
      " |-- del_add_ratio: double (nullable = true)\n",
      " |-- profanity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\"title_page, name_user\", \"comment\", \"url_page\", \"deletions\"]\n",
    "wiki_test = wiki_test.drop(*columns_to_drop)\n",
    "print(type(wiki_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???\n",
    "features_array = wiki_test.selectExpr(\"\"\"array(\n",
    "CAST(additions AS FLOAT),\n",
    "CAST(del_add_ratio AS FLOAT), \n",
    "CAST(profanity AS FLOAT)\n",
    ") as arr\"\"\", \n",
    "                                      \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization of streaming data\n",
    "tovec_udf = funcs.udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "data_stream = features_array.withColumn(\"features\", tovec_udf(\"arr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction of Streaming Data\n",
    "prediction = modeldt.transform(data_stream)\n",
    "type(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arr: array (nullable = false)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Sliding Window Time using Current Timestamp\n",
    "currentTimeDf = prediction.withColumn(\"processingTime\",funcs.current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 - Using Sliding Windows and Watermarking (Confusion Matrix)\n",
    "confusion_matrix = currentTimeDf\\\n",
    ".withWatermark(\"processingTime\", \"5 seconds\")\\\n",
    ".groupBy(funcs.window(\"processingTime\", \"3 seconds\", \"1 seconds\"),\"label\", \"prediction\")\\\n",
    ".count()\\\n",
    ".withColumn(\"prediction\",funcs.when(funcs.col(\"label\").isin(0), \"safe\")\\\n",
    "    .when(funcs.col(\"label\").isin(1), \"unsafe\") \\\n",
    ".otherwise(\"vandal\"))\\\n",
    ".orderBy(\"window\") #.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = confusion_matrix.writeStream\\\n",
    ".outputMode(\"complete\")\\\n",
    ".format(\"console\")\\\n",
    ".option(\"truncate\", \"false\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "'Option \\'basePath\\' must be a directory\\n=== Streaming Query ===\\nIdentifier: [id = b52671cf-62fc-4b98-8d85-d4ef9a8fc1e5, runId = 93c44f3f-39ba-41a9-818f-3a825b4c6330]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {FileStreamSource[file:/C:/Users/Hans/PhD/Course_Data_Analytics/Assignment_3/spark/notebooks/My_Notebooks/Wikipedia.csv]: {\"logOffset\":0}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nSort [window#1026-T5000ms ASC NULLS FIRST], true\\n+- Project [window#1026-T5000ms, label#921, CASE WHEN cast(label#921 as string) IN (cast(0 as string)) THEN safe WHEN cast(label#921 as string) IN (cast(1 as string)) THEN unsafe ELSE vandal END AS prediction#1041, count#1035L]\\n   +- Aggregate [window#1036-T5000ms, label#921, prediction#962], [window#1036-T5000ms AS window#1026-T5000ms, label#921, prediction#962, count(1) AS count#1035L]\\n      +- Filter ((processingTime#975-T5000ms >= window#1036-T5000ms.start) && (processingTime#975-T5000ms < window#1036-T5000ms.end))\\n         +- Expand [ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms)], [window#1036-T5000ms, arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms]\\n            +- EventTimeWatermark processingTime#975: timestamp, interval 5 seconds\\n               +- Project [arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, current_timestamp() AS processingTime#975]\\n                  +- Project [arr#943, label#921, features#947, rawPrediction#951, probability#956, UDF(rawPrediction#951) AS prediction#962]\\n                     +- Project [arr#943, label#921, features#947, rawPrediction#951, UDF(rawPrediction#951) AS probability#956]\\n                        +- Project [arr#943, label#921, features#947, UDF(features#947) AS rawPrediction#951]\\n                           +- Project [arr#943, label#921, <lambda>(arr#943) AS features#947]\\n                              +- Project [array(cast(additions#925 as float), cast(del_add_ratio#926 as float), cast(profanity#927 as float)) AS arr#943, label#921]\\n                                 +- Project [title_page#919, name_user#920, label#921, additions#925, del_add_ratio#926, profanity#927]\\n                                    +- StreamingExecutionRelation FileStreamSource[file:/C:/Users/Hans/PhD/Course_Data_Analytics/Assignment_3/spark/notebooks/My_Notebooks/Wikipedia.csv], [title_page#919, name_user#920, label#921, comment#922, url_page#923, deletions#924, additions#925, del_add_ratio#926, profanity#927]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\PhD\\Course_Data_Analytics\\Assignment_3\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PhD\\Course_Data_Analytics\\Assignment_3\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2090.awaitTermination.\n: org.apache.spark.sql.streaming.StreamingQueryException: Option 'basePath' must be a directory\n=== Streaming Query ===\nIdentifier: [id = b52671cf-62fc-4b98-8d85-d4ef9a8fc1e5, runId = 93c44f3f-39ba-41a9-818f-3a825b4c6330]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/C:/Users/Hans/PhD/Course_Data_Analytics/Assignment_3/spark/notebooks/My_Notebooks/Wikipedia.csv]: {\"logOffset\":0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nSort [window#1026-T5000ms ASC NULLS FIRST], true\n+- Project [window#1026-T5000ms, label#921, CASE WHEN cast(label#921 as string) IN (cast(0 as string)) THEN safe WHEN cast(label#921 as string) IN (cast(1 as string)) THEN unsafe ELSE vandal END AS prediction#1041, count#1035L]\n   +- Aggregate [window#1036-T5000ms, label#921, prediction#962], [window#1036-T5000ms AS window#1026-T5000ms, label#921, prediction#962, count(1) AS count#1035L]\n      +- Filter ((processingTime#975-T5000ms >= window#1036-T5000ms.start) && (processingTime#975-T5000ms < window#1036-T5000ms.end))\n         +- Expand [ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms)], [window#1036-T5000ms, arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms]\n            +- EventTimeWatermark processingTime#975: timestamp, interval 5 seconds\n               +- Project [arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, current_timestamp() AS processingTime#975]\n                  +- Project [arr#943, label#921, features#947, rawPrediction#951, probability#956, UDF(rawPrediction#951) AS prediction#962]\n                     +- Project [arr#943, label#921, features#947, rawPrediction#951, UDF(rawPrediction#951) AS probability#956]\n                        +- Project [arr#943, label#921, features#947, UDF(features#947) AS rawPrediction#951]\n                           +- Project [arr#943, label#921, <lambda>(arr#943) AS features#947]\n                              +- Project [array(cast(additions#925 as float), cast(del_add_ratio#926 as float), cast(profanity#927 as float)) AS arr#943, label#921]\n                                 +- Project [title_page#919, name_user#920, label#921, additions#925, del_add_ratio#926, profanity#927]\n                                    +- StreamingExecutionRelation FileStreamSource[file:/C:/Users/Hans/PhD/Course_Data_Analytics/Assignment_3/spark/notebooks/My_Notebooks/Wikipedia.csv], [title_page#919, name_user#920, label#921, comment#922, url_page#923, deletions#924, additions#925, del_add_ratio#926, profanity#927]\n\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:297)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)\r\nCaused by: java.lang.IllegalArgumentException: Option 'basePath' must be a directory\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.basePaths(PartitioningAwareFileIndex.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:134)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:71)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:144)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.getBatch(FileStreamSource.scala:174)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$3$$anonfun$apply$10.apply(MicroBatchExecution.scala:438)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$3$$anonfun$apply$10.apply(MicroBatchExecution.scala:434)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\r\n\tat org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:25)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat org.apache.spark.sql.execution.streaming.StreamProgress.flatMap(StreamProgress.scala:25)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$3.apply(MicroBatchExecution.scala:434)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$3.apply(MicroBatchExecution.scala:434)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:433)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)\r\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)\r\n\t... 1 more\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-b1d86780078a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PhD\\Course_Data_Analytics\\Assignment_3\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PhD\\Course_Data_Analytics\\Assignment_3\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PhD\\Course_Data_Analytics\\Assignment_3\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.execution.QueryExecutionException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: 'Option \\'basePath\\' must be a directory\\n=== Streaming Query ===\\nIdentifier: [id = b52671cf-62fc-4b98-8d85-d4ef9a8fc1e5, runId = 93c44f3f-39ba-41a9-818f-3a825b4c6330]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {FileStreamSource[file:/C:/Users/Hans/PhD/Course_Data_Analytics/Assignment_3/spark/notebooks/My_Notebooks/Wikipedia.csv]: {\"logOffset\":0}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nSort [window#1026-T5000ms ASC NULLS FIRST], true\\n+- Project [window#1026-T5000ms, label#921, CASE WHEN cast(label#921 as string) IN (cast(0 as string)) THEN safe WHEN cast(label#921 as string) IN (cast(1 as string)) THEN unsafe ELSE vandal END AS prediction#1041, count#1035L]\\n   +- Aggregate [window#1036-T5000ms, label#921, prediction#962], [window#1036-T5000ms AS window#1026-T5000ms, label#921, prediction#962, count(1) AS count#1035L]\\n      +- Filter ((processingTime#975-T5000ms >= window#1036-T5000ms.start) && (processingTime#975-T5000ms < window#1036-T5000ms.end))\\n         +- Expand [ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 1000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) as double) = (cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) THEN (CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(processingTime#975-T5000ms, TimestampType, LongType) - 0) as double) / cast(1000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 1000000) + 0) + 3000000), LongType, TimestampType)), arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms)], [window#1036-T5000ms, arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, processingTime#975-T5000ms]\\n            +- EventTimeWatermark processingTime#975: timestamp, interval 5 seconds\\n               +- Project [arr#943, label#921, features#947, rawPrediction#951, probability#956, prediction#962, current_timestamp() AS processingTime#975]\\n                  +- Project [arr#943, label#921, features#947, rawPrediction#951, probability#956, UDF(rawPrediction#951) AS prediction#962]\\n                     +- Project [arr#943, label#921, features#947, rawPrediction#951, UDF(rawPrediction#951) AS probability#956]\\n                        +- Project [arr#943, label#921, features#947, UDF(features#947) AS rawPrediction#951]\\n                           +- Project [arr#943, label#921, <lambda>(arr#943) AS features#947]\\n                              +- Project [array(cast(additions#925 as float), cast(del_add_ratio#926 as float), cast(profanity#927 as float)) AS arr#943, label#921]\\n                                 +- Project [title_page#919, name_user#920, label#921, additions#925, del_add_ratio#926, profanity#927]\\n                                    +- StreamingExecutionRelation FileStreamSource[file:/C:/Users/Hans/PhD/Course_Data_Analytics/Assignment_3/spark/notebooks/My_Notebooks/Wikipedia.csv], [title_page#919, name_user#920, label#921, comment#922, url_page#923, deletions#924, additions#925, del_add_ratio#926, profanity#927]\\n'"
     ]
    }
   ],
   "source": [
    "# werkt niet, valt niet te stoppen\n",
    "q.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 - Using Append method\n",
    "prediction = prediction.select(\"features\",\"label\",\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = prediction.writeStream\\\n",
    ".outputMode(\"append\")\\\n",
    ".format(\"console\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ik probeer naar file te schrijven\n",
    "q = prediction.writeStream  \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", r\"C:/Users/Hans/PhD/Course_Data_Analytics/Assignment_3/spark/notebooks/Results\")\\\n",
    "    .option(\"checkpointLocation\", \"src/main/resources/chkpoint_dir\")\n",
    "    #.awaitTermination()\n",
    "q.start()\n",
    "#.option(\"checkpointLocation\", \"src/main/resources/chkpoint_dir\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# werkt niet, valt niet stoppen\n",
    "q.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
