{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from difflib import unified_diff\n",
    "from pyspark import ml\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, NaiveBayes\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"assignment3\")\n",
    "spark = SparkSession.builder.appName('assignment3').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs-1585746250000',\n",
       " 'outputs-1585746260000',\n",
       " 'outputs-1585746270000',\n",
       " 'outputs-1585746280000',\n",
       " 'outputs-1585746290000',\n",
       " 'outputs-1585746300000',\n",
       " 'outputs-1585746310000',\n",
       " 'outputs-1585746320000',\n",
       " 'outputs-1585746330000',\n",
       " 'outputs-1585746340000',\n",
       " 'outputs-1585746350000',\n",
       " 'outputs-1585746360000',\n",
       " 'outputs-1585746370000',\n",
       " 'outputs-1585746380000',\n",
       " 'outputs-1585746390000',\n",
       " 'outputs-1585746400000',\n",
       " 'outputs-1585746410000',\n",
       " 'outputs-1585746420000',\n",
       " 'outputs-1585746430000',\n",
       " 'outputs-1585746440000',\n",
       " 'outputs-1585746450000',\n",
       " 'outputs-1585746460000',\n",
       " 'outputs-1585746470000',\n",
       " 'outputs-1585746480000',\n",
       " 'outputs-1585746490000',\n",
       " 'outputs-1585746500000',\n",
       " 'outputs-1585746510000',\n",
       " 'outputs-1585746520000',\n",
       " 'outputs-1585746530000',\n",
       " 'outputs-1585746540000',\n",
       " 'outputs-1585746550000',\n",
       " 'outputs-1585746560000',\n",
       " 'outputs-1585746570000',\n",
       " 'outputs-1585746580000',\n",
       " 'outputs-1585746590000',\n",
       " 'outputs-1585746600000',\n",
       " 'outputs-1585746610000',\n",
       " 'outputs-1585746620000',\n",
       " 'outputs-1585746630000',\n",
       " 'outputs-1585746640000',\n",
       " 'outputs-1585746650000',\n",
       " 'outputs-1585746660000',\n",
       " 'outputs-1585746670000',\n",
       " 'outputs-1585746680000',\n",
       " 'outputs-1585746690000',\n",
       " 'outputs-1585746700000',\n",
       " 'outputs-1585746710000',\n",
       " 'outputs-1585746720000',\n",
       " 'outputs-1585746730000',\n",
       " 'outputs-1585746740000',\n",
       " 'outputs-1585746750000',\n",
       " 'outputs-1585746760000',\n",
       " 'outputs-1585746770000',\n",
       " 'outputs-1585746780000',\n",
       " 'outputs-1585746790000',\n",
       " 'outputs-1585746800000',\n",
       " 'outputs-1585746810000',\n",
       " 'outputs-1585746820000',\n",
       " 'outputs-1585746830000',\n",
       " 'outputs-1585746840000',\n",
       " 'outputs-1585746850000',\n",
       " 'outputs-1585746860000',\n",
       " 'outputs-1585746870000',\n",
       " 'outputs-1585746880000',\n",
       " 'outputs-1585746890000',\n",
       " 'outputs-1585746900000',\n",
       " 'outputs-1585746910000',\n",
       " 'outputs-1585746920000',\n",
       " 'outputs-1585746930000',\n",
       " 'outputs-1585746940000',\n",
       " 'outputs-1585746950000',\n",
       " 'outputs-1585746960000',\n",
       " 'outputs-1585746970000',\n",
       " 'outputs-1585746980000',\n",
       " 'outputs-1585746990000',\n",
       " 'outputs-1585747000000',\n",
       " 'outputs-1585747010000',\n",
       " 'outputs-1585747020000',\n",
       " 'outputs-1585747030000',\n",
       " 'outputs-1585747040000',\n",
       " 'outputs-1585747050000',\n",
       " 'outputs-1585747060000',\n",
       " 'outputs-1585747070000',\n",
       " 'outputs-1585747080000',\n",
       " 'outputs-1585747090000',\n",
       " 'outputs-1585747100000',\n",
       " 'outputs-1585747110000',\n",
       " 'outputs-1585747120000',\n",
       " 'outputs-1585747130000',\n",
       " 'outputs-1585747140000',\n",
       " 'outputs-1585747150000',\n",
       " 'outputs-1585747160000',\n",
       " 'outputs-1585747170000',\n",
       " 'outputs-1585747180000',\n",
       " 'outputs-1585747190000',\n",
       " 'outputs-1585747200000',\n",
       " 'outputs-1585747210000',\n",
       " 'outputs-1585747220000',\n",
       " 'outputs-1585747230000',\n",
       " 'outputs-1585747240000',\n",
       " 'outputs-1585747250000',\n",
       " 'outputs-1585747260000',\n",
       " 'outputs-1585747270000',\n",
       " 'outputs-1585747280000',\n",
       " 'outputs-1585747290000',\n",
       " 'outputs-1585747300000',\n",
       " 'outputs-1585747310000',\n",
       " 'outputs-1585747320000',\n",
       " 'outputs-1585747330000',\n",
       " 'outputs-1585747340000',\n",
       " 'outputs-1585747350000',\n",
       " 'outputs-1585747360000',\n",
       " 'outputs-1585747370000',\n",
       " 'outputs-1585747380000',\n",
       " 'outputs-1585747390000',\n",
       " 'outputs-1585747400000',\n",
       " 'outputs-1585747410000',\n",
       " 'outputs-1585747420000',\n",
       " 'outputs-1585747430000',\n",
       " 'outputs-1585747440000',\n",
       " 'outputs-1585747450000',\n",
       " 'outputs-1585747460000',\n",
       " 'outputs-1585747470000',\n",
       " 'outputs-1585747480000',\n",
       " 'outputs-1585747490000',\n",
       " 'outputs-1585747500000',\n",
       " 'outputs-1585747510000',\n",
       " 'outputs-1585747520000',\n",
       " 'outputs-1585747530000',\n",
       " 'outputs-1585747540000',\n",
       " 'outputs-1585747550000',\n",
       " 'outputs-1585747560000',\n",
       " 'outputs-1585747570000',\n",
       " 'outputs-1585747580000',\n",
       " 'outputs-1585747590000',\n",
       " 'outputs-1585747600000',\n",
       " 'outputs-1585747610000',\n",
       " 'outputs-1585747620000',\n",
       " 'outputs-1585747630000',\n",
       " 'outputs-1585747640000',\n",
       " 'outputs-1585747650000',\n",
       " 'outputs-1585747660000',\n",
       " 'outputs-1585747670000',\n",
       " 'outputs-1585747680000',\n",
       " 'outputs-1585747690000',\n",
       " 'outputs-1585747700000',\n",
       " 'outputs-1585747710000',\n",
       " 'outputs-1585747720000',\n",
       " 'outputs-1585747730000',\n",
       " 'outputs-1585747740000',\n",
       " 'outputs-1585747750000',\n",
       " 'outputs-1585747760000',\n",
       " 'outputs-1585747770000',\n",
       " 'outputs-1585747780000',\n",
       " 'outputs-1585747790000',\n",
       " 'outputs-1585747800000',\n",
       " 'outputs-1585747810000',\n",
       " 'outputs-1585747820000',\n",
       " 'outputs-1585747830000',\n",
       " 'outputs-1585747840000',\n",
       " 'outputs-1585747850000',\n",
       " 'outputs-1585747860000',\n",
       " 'outputs-1585747870000',\n",
       " 'outputs-1585747880000',\n",
       " 'outputs-1585747890000',\n",
       " 'outputs-1585747900000',\n",
       " 'outputs-1585747910000',\n",
       " 'outputs-1585747920000',\n",
       " 'outputs-1585747930000',\n",
       " 'outputs-1585747940000',\n",
       " 'outputs-1585747950000',\n",
       " 'outputs-1585747960000',\n",
       " 'outputs-1585747970000',\n",
       " 'outputs-1585747980000',\n",
       " 'outputs-1585747990000',\n",
       " 'outputs-1585748000000',\n",
       " 'outputs-1585748010000',\n",
       " 'outputs-1585748020000',\n",
       " 'outputs-1585748030000',\n",
       " 'outputs-1585748040000',\n",
       " 'outputs-1585748050000',\n",
       " 'outputs-1585748060000',\n",
       " 'outputs-1585748070000',\n",
       " 'outputs-1585748080000',\n",
       " 'outputs-1585748090000',\n",
       " 'outputs-1585748100000',\n",
       " 'outputs-1585748110000',\n",
       " 'outputs-1585748120000',\n",
       " 'outputs-1585748130000',\n",
       " 'outputs-1585748140000',\n",
       " 'outputs-1585748150000',\n",
       " 'outputs-1585748160000',\n",
       " 'outputs-1585748170000',\n",
       " 'outputs-1585748180000',\n",
       " 'outputs-1585748190000',\n",
       " 'outputs-1585748200000',\n",
       " 'outputs-1585748210000',\n",
       " 'outputs-1585748220000',\n",
       " 'outputs-1585748230000',\n",
       " 'outputs-1585748240000',\n",
       " 'outputs-1585748250000',\n",
       " 'outputs-1585748260000',\n",
       " 'outputs-1585748270000',\n",
       " 'outputs-1585748280000',\n",
       " 'outputs-1585748290000',\n",
       " 'outputs-1585748300000',\n",
       " 'outputs-1585748310000',\n",
       " 'outputs-1585748320000',\n",
       " 'outputs-1585748330000',\n",
       " 'outputs-1585748340000',\n",
       " 'outputs-1585748350000',\n",
       " 'outputs-1585748360000',\n",
       " 'outputs-1585748370000',\n",
       " 'outputs-1585748380000',\n",
       " 'outputs-1585748390000',\n",
       " 'outputs-1585748400000',\n",
       " 'outputs-1585748410000',\n",
       " 'outputs-1585748420000',\n",
       " 'outputs-1585748430000',\n",
       " 'outputs-1585748440000',\n",
       " 'outputs-1585748450000',\n",
       " 'outputs-1585748460000',\n",
       " 'outputs-1585748470000',\n",
       " 'outputs-1585748480000',\n",
       " 'outputs-1585748490000',\n",
       " 'outputs-1585748500000',\n",
       " 'outputs-1585748510000',\n",
       " 'outputs-1585748520000',\n",
       " 'outputs-1585748530000',\n",
       " 'outputs-1585748540000',\n",
       " 'outputs-1585748550000',\n",
       " 'outputs-1585748560000',\n",
       " 'outputs-1585748570000',\n",
       " 'outputs-1585748580000',\n",
       " 'outputs-1585748590000',\n",
       " 'outputs-1585748600000',\n",
       " 'outputs-1585748610000',\n",
       " 'outputs-1585748620000',\n",
       " 'outputs-1585748630000',\n",
       " 'outputs-1585748640000',\n",
       " 'outputs-1585748650000',\n",
       " 'outputs-1585748660000',\n",
       " 'outputs-1585748670000',\n",
       " 'outputs-1585748680000',\n",
       " 'outputs-1585748690000',\n",
       " 'outputs-1585748700000',\n",
       " 'outputs-1585748710000',\n",
       " 'outputs-1585748720000',\n",
       " 'outputs-1585748730000',\n",
       " 'outputs-1585748740000',\n",
       " 'outputs-1585748750000',\n",
       " 'outputs-1585748760000',\n",
       " 'outputs-1585748770000',\n",
       " 'outputs-1585748780000',\n",
       " 'outputs-1585748790000',\n",
       " 'outputs-1585748800000',\n",
       " 'outputs-1585748810000',\n",
       " 'outputs-1585748820000',\n",
       " 'outputs-1585748830000',\n",
       " 'outputs-1585748840000',\n",
       " 'outputs-1585748850000',\n",
       " 'outputs-1585748860000',\n",
       " 'outputs-1585748870000',\n",
       " 'outputs-1585748880000',\n",
       " 'outputs-1585748890000',\n",
       " 'outputs-1585748900000',\n",
       " 'outputs-1585748910000',\n",
       " 'outputs-1585748920000',\n",
       " 'outputs-1585748930000',\n",
       " 'outputs-1585748940000',\n",
       " 'outputs-1585748950000',\n",
       " 'outputs-1585748960000',\n",
       " 'outputs-1585748970000',\n",
       " 'outputs-1585748980000',\n",
       " 'outputs-1585748990000',\n",
       " 'outputs-1585749000000',\n",
       " 'outputs-1585749010000',\n",
       " 'outputs-1585749020000',\n",
       " 'outputs-1585749030000',\n",
       " 'outputs-1585749040000',\n",
       " 'outputs-1585749050000',\n",
       " 'outputs-1585749060000',\n",
       " 'outputs-1585749070000',\n",
       " 'outputs-1585749080000',\n",
       " 'outputs-1585749090000',\n",
       " 'outputs-1585749100000',\n",
       " 'outputs-1585749110000',\n",
       " 'outputs-1585749120000',\n",
       " 'outputs-1585749130000',\n",
       " 'outputs-1585749140000',\n",
       " 'outputs-1585749150000',\n",
       " 'outputs-1585749160000',\n",
       " 'outputs-1585749170000',\n",
       " 'outputs-1585749180000',\n",
       " 'outputs-1585749190000',\n",
       " 'outputs-1585749200000',\n",
       " 'outputs-1585749210000',\n",
       " 'outputs-1585749220000',\n",
       " 'outputs-1585749230000',\n",
       " 'outputs-1585749240000',\n",
       " 'outputs-1585749250000',\n",
       " 'outputs-1585749260000',\n",
       " 'outputs-1585749270000',\n",
       " 'outputs-1585749280000',\n",
       " 'outputs-1585749290000',\n",
       " 'outputs-1585749300000',\n",
       " 'outputs-1585749310000',\n",
       " 'outputs-1585749320000',\n",
       " 'outputs-1585749330000',\n",
       " 'outputs-1585749340000',\n",
       " 'outputs-1585749350000',\n",
       " 'outputs-1585749360000',\n",
       " 'outputs-1585749370000',\n",
       " 'outputs-1585749380000',\n",
       " 'outputs-1585749390000',\n",
       " 'outputs-1585749400000',\n",
       " 'outputs-1585749410000',\n",
       " 'outputs-1585749420000',\n",
       " 'outputs-1585749430000',\n",
       " 'outputs-1585749440000',\n",
       " 'outputs-1585749450000',\n",
       " 'outputs-1585749460000',\n",
       " 'outputs-1585749470000',\n",
       " 'outputs-1585749480000',\n",
       " 'outputs-1585749490000',\n",
       " 'outputs-1585749500000',\n",
       " 'outputs-1585749510000',\n",
       " 'outputs-1585749520000',\n",
       " 'outputs-1585749530000',\n",
       " 'outputs-1585749540000',\n",
       " 'outputs-1585749550000',\n",
       " 'outputs-1585749560000',\n",
       " 'outputs-1585749570000',\n",
       " 'outputs-1585749580000',\n",
       " 'outputs-1585749590000',\n",
       " 'outputs-1585749600000',\n",
       " 'outputs-1585749610000',\n",
       " 'outputs-1585749620000',\n",
       " 'outputs-1585749630000',\n",
       " 'outputs-1585749640000',\n",
       " 'outputs-1585749650000',\n",
       " 'outputs-1585749660000',\n",
       " 'outputs-1585749670000',\n",
       " 'outputs-1585749680000',\n",
       " 'outputs-1585749690000',\n",
       " 'outputs-1585749700000',\n",
       " 'outputs-1585749710000',\n",
       " 'outputs-1585749720000',\n",
       " 'outputs-1585749730000',\n",
       " 'outputs-1585749740000',\n",
       " 'outputs-1585749750000',\n",
       " 'outputs-1585749760000',\n",
       " 'outputs-1585749770000',\n",
       " 'outputs-1585749780000',\n",
       " 'outputs-1585749790000',\n",
       " 'outputs-1585749800000',\n",
       " 'outputs-1585749810000',\n",
       " 'outputs-1585749820000',\n",
       " 'outputs-1585749830000',\n",
       " 'outputs-1585749840000',\n",
       " 'outputs-1585749850000',\n",
       " 'outputs-1585749860000',\n",
       " 'outputs-1585749870000',\n",
       " 'outputs-1585749880000',\n",
       " 'outputs-1585749890000',\n",
       " 'outputs-1585749900000',\n",
       " 'outputs-1585749910000',\n",
       " 'outputs-1585749920000',\n",
       " 'outputs-1585749930000',\n",
       " 'outputs-1585749940000',\n",
       " 'outputs-1585749950000',\n",
       " 'outputs-1585749960000',\n",
       " 'outputs-1585749970000',\n",
       " 'outputs-1585749980000',\n",
       " 'outputs-1585749990000',\n",
       " 'outputs-1585750000000',\n",
       " 'outputs-1585750010000',\n",
       " 'outputs-1585750020000',\n",
       " 'outputs-1585750030000',\n",
       " 'outputs-1585750040000',\n",
       " 'outputs-1585750050000',\n",
       " 'outputs-1585750060000',\n",
       " 'outputs-1585750070000',\n",
       " 'outputs-1585750080000',\n",
       " 'outputs-1585750090000',\n",
       " 'outputs-1585750100000',\n",
       " 'outputs-1585750110000',\n",
       " 'outputs-1585750120000',\n",
       " 'outputs-1585750130000',\n",
       " 'outputs-1585750140000',\n",
       " 'outputs-1585750150000',\n",
       " 'outputs-1585750160000',\n",
       " 'outputs-1585750170000',\n",
       " 'outputs-1585750180000',\n",
       " 'outputs-1585750190000',\n",
       " 'outputs-1585750200000',\n",
       " 'outputs-1585750210000',\n",
       " 'outputs-1585750220000',\n",
       " 'outputs-1585750230000',\n",
       " 'outputs-1585750240000',\n",
       " 'outputs-1585750250000',\n",
       " 'outputs-1585750260000',\n",
       " 'outputs-1585750270000',\n",
       " 'outputs-1585750280000',\n",
       " 'outputs-1585750290000',\n",
       " 'outputs-1585750300000',\n",
       " 'outputs-1585750310000',\n",
       " 'outputs-1585750320000',\n",
       " 'outputs-1585750330000',\n",
       " 'outputs-1585750340000',\n",
       " 'outputs-1585750350000',\n",
       " 'outputs-1585750360000',\n",
       " 'outputs-1585750370000',\n",
       " 'outputs-1585750380000',\n",
       " 'outputs-1585750390000',\n",
       " 'outputs-1585750400000',\n",
       " 'outputs-1585750410000',\n",
       " 'outputs-1585750420000',\n",
       " 'outputs-1585750430000',\n",
       " 'outputs-1585750440000',\n",
       " 'outputs-1585750450000',\n",
       " 'outputs-1585750460000',\n",
       " 'outputs-1585750470000',\n",
       " 'outputs-1585750480000',\n",
       " 'outputs-1585750490000',\n",
       " 'outputs-1585750500000',\n",
       " 'outputs-1585750510000',\n",
       " 'outputs-1585750520000',\n",
       " 'outputs-1585750530000',\n",
       " 'outputs-1585750540000',\n",
       " 'outputs-1585750550000',\n",
       " 'outputs-1585750560000',\n",
       " 'outputs-1585750570000',\n",
       " 'outputs-1585750580000',\n",
       " 'outputs-1585750590000',\n",
       " 'outputs-1585750600000',\n",
       " 'outputs-1585750610000',\n",
       " 'outputs-1585750620000',\n",
       " 'outputs-1585750630000',\n",
       " 'outputs-1585750640000',\n",
       " 'outputs-1585750650000',\n",
       " 'outputs-1585750660000',\n",
       " 'outputs-1585750670000',\n",
       " 'outputs-1585750680000',\n",
       " 'outputs-1585750690000',\n",
       " 'outputs-1585750700000',\n",
       " 'outputs-1585750710000',\n",
       " 'outputs-1585750720000',\n",
       " 'outputs-1585750730000',\n",
       " 'outputs-1585750740000',\n",
       " 'outputs-1585750750000',\n",
       " 'outputs-1585750760000',\n",
       " 'outputs-1585750770000',\n",
       " 'outputs-1585750780000',\n",
       " 'outputs-1585750790000',\n",
       " 'outputs-1585750800000',\n",
       " 'outputs-1585750810000',\n",
       " 'outputs-1585750820000',\n",
       " 'outputs-1585750830000',\n",
       " 'outputs-1585750840000',\n",
       " 'outputs-1585750850000',\n",
       " 'outputs-1585750860000',\n",
       " 'outputs-1585750870000',\n",
       " 'outputs-1585750880000',\n",
       " 'outputs-1585750890000',\n",
       " 'outputs-1585750900000',\n",
       " 'outputs-1585750910000',\n",
       " 'outputs-1585750920000',\n",
       " 'outputs-1585750930000',\n",
       " 'outputs-1585750940000',\n",
       " 'outputs-1585750950000',\n",
       " 'outputs-1585750960000',\n",
       " 'outputs-1585750970000',\n",
       " 'outputs-1585750980000',\n",
       " 'outputs-1585750990000',\n",
       " 'outputs-1585751000000',\n",
       " 'outputs-1585751010000',\n",
       " 'outputs-1585751020000',\n",
       " 'outputs-1585751030000',\n",
       " 'outputs-1585751040000',\n",
       " 'outputs-1585751050000',\n",
       " 'outputs-1585751060000',\n",
       " 'outputs-1585751070000',\n",
       " 'outputs-1585751080000',\n",
       " 'outputs-1585751090000',\n",
       " 'outputs-1585751100000',\n",
       " 'outputs-1585751110000',\n",
       " 'outputs-1585751120000',\n",
       " 'outputs-1585751130000',\n",
       " 'outputs-1585751140000',\n",
       " 'outputs-1585751150000',\n",
       " 'outputs-1585751160000',\n",
       " 'outputs-1585751170000',\n",
       " 'outputs-1585751180000',\n",
       " 'outputs-1585751190000',\n",
       " 'outputs-1585751200000',\n",
       " 'outputs-1585751210000',\n",
       " 'outputs-1585751220000',\n",
       " 'outputs-1585751230000',\n",
       " 'outputs-1585751240000',\n",
       " 'outputs-1585751250000',\n",
       " 'outputs-1585751260000',\n",
       " 'outputs-1585751270000',\n",
       " 'outputs-1585751280000',\n",
       " 'outputs-1585751290000',\n",
       " 'outputs-1585751300000',\n",
       " 'outputs-1585751310000',\n",
       " 'outputs-1585751320000',\n",
       " 'outputs-1585751330000',\n",
       " 'outputs-1585751340000',\n",
       " 'outputs-1585751350000',\n",
       " 'outputs-1585751360000',\n",
       " 'outputs-1585751370000',\n",
       " 'outputs-1585751380000',\n",
       " 'outputs-1585751390000',\n",
       " 'outputs-1585751400000',\n",
       " 'outputs-1585751410000',\n",
       " 'outputs-1585751420000',\n",
       " 'outputs-1585751430000',\n",
       " 'outputs-1585751440000',\n",
       " 'outputs-1585751450000',\n",
       " 'outputs-1585751460000',\n",
       " 'outputs-1585751470000',\n",
       " 'outputs-1585751480000',\n",
       " 'outputs-1585751490000',\n",
       " 'outputs-1585751500000',\n",
       " 'outputs-1585751510000',\n",
       " 'outputs-1585751520000',\n",
       " 'outputs-1585751530000',\n",
       " 'outputs-1585751540000',\n",
       " 'outputs-1585751550000',\n",
       " 'outputs-1585751560000',\n",
       " 'outputs-1585751570000',\n",
       " 'outputs-1585751580000',\n",
       " 'outputs-1585751590000',\n",
       " 'outputs-1585751600000',\n",
       " 'outputs-1585751610000',\n",
       " 'outputs-1585751620000',\n",
       " 'outputs-1585751630000',\n",
       " 'outputs-1585751640000',\n",
       " 'outputs-1585751650000',\n",
       " 'outputs-1585751660000',\n",
       " 'outputs-1585751670000',\n",
       " 'outputs-1585751680000',\n",
       " 'outputs-1585751690000',\n",
       " 'outputs-1585751700000',\n",
       " 'outputs-1585751710000',\n",
       " 'outputs-1585751720000',\n",
       " 'outputs-1585751730000',\n",
       " 'outputs-1585751740000',\n",
       " 'outputs-1585751750000',\n",
       " 'outputs-1585751760000',\n",
       " 'outputs-1585751770000',\n",
       " 'outputs-1585751780000',\n",
       " 'outputs-1585751790000',\n",
       " 'outputs-1585751800000',\n",
       " 'outputs-1585751810000',\n",
       " 'outputs-1585751820000',\n",
       " 'outputs-1585751830000',\n",
       " 'outputs-1585751840000',\n",
       " 'outputs-1585751850000',\n",
       " 'outputs-1585751860000',\n",
       " 'outputs-1585751870000',\n",
       " 'outputs-1585751880000',\n",
       " 'outputs-1585751890000',\n",
       " 'outputs-1585751900000',\n",
       " 'outputs-1585751910000',\n",
       " 'outputs-1585751920000',\n",
       " 'outputs-1585751930000',\n",
       " 'outputs-1585751940000',\n",
       " 'outputs-1585751950000',\n",
       " 'outputs-1585751960000',\n",
       " 'outputs-1585751970000',\n",
       " 'outputs-1585751980000',\n",
       " 'outputs-1585751990000',\n",
       " 'outputs-1585752000000',\n",
       " 'outputs-1585752010000',\n",
       " 'outputs-1585752020000',\n",
       " 'outputs-1585752030000',\n",
       " 'outputs-1585752040000',\n",
       " 'outputs-1585752050000',\n",
       " 'outputs-1585752060000',\n",
       " 'outputs-1585752070000',\n",
       " 'outputs-1585752080000',\n",
       " 'outputs-1585752090000',\n",
       " 'outputs-1585752100000',\n",
       " 'outputs-1585752110000',\n",
       " 'outputs-1585752120000',\n",
       " 'outputs-1585752130000',\n",
       " 'outputs-1585752140000',\n",
       " 'outputs-1585752150000',\n",
       " 'outputs-1585752160000',\n",
       " 'outputs-1585752170000',\n",
       " 'outputs-1585752180000',\n",
       " 'outputs-1585752190000',\n",
       " 'outputs-1585752200000',\n",
       " 'outputs-1585752210000',\n",
       " 'outputs-1585752220000',\n",
       " 'outputs-1585752230000',\n",
       " 'outputs-1585752240000',\n",
       " 'outputs-1585752250000',\n",
       " 'outputs-1585752260000',\n",
       " 'outputs-1585752270000',\n",
       " 'outputs-1585752280000',\n",
       " 'outputs-1585752290000',\n",
       " 'outputs-1585752300000',\n",
       " 'outputs-1585752310000',\n",
       " 'outputs-1585752320000',\n",
       " 'outputs-1585752330000',\n",
       " 'outputs-1585752340000',\n",
       " 'outputs-1585752350000',\n",
       " 'outputs-1585752360000',\n",
       " 'outputs-1585752370000',\n",
       " 'outputs-1585752380000',\n",
       " 'outputs-1585752390000',\n",
       " 'outputs-1585752400000',\n",
       " 'outputs-1585752410000',\n",
       " 'outputs-1585752420000',\n",
       " 'outputs-1585752430000',\n",
       " 'outputs-1585752440000',\n",
       " 'outputs-1585752450000',\n",
       " 'outputs-1585752460000',\n",
       " 'outputs-1585752470000',\n",
       " 'outputs-1585752480000',\n",
       " 'outputs-1585752490000',\n",
       " 'outputs-1585752500000',\n",
       " 'outputs-1585752510000',\n",
       " 'outputs-1585752520000',\n",
       " 'outputs-1585752530000',\n",
       " 'outputs-1585752540000',\n",
       " 'outputs-1585752550000',\n",
       " 'outputs-1585752560000',\n",
       " 'outputs-1585752570000',\n",
       " 'outputs-1585752580000',\n",
       " 'outputs-1585752590000',\n",
       " 'outputs-1585752600000',\n",
       " 'outputs-1585752610000',\n",
       " 'outputs-1585752620000',\n",
       " 'outputs-1585752630000',\n",
       " 'outputs-1585752640000',\n",
       " 'outputs-1585752650000',\n",
       " 'outputs-1585752660000',\n",
       " 'outputs-1585752670000',\n",
       " 'outputs-1585752680000',\n",
       " 'outputs-1585752690000',\n",
       " 'outputs-1585752700000',\n",
       " 'outputs-1585752710000',\n",
       " 'outputs-1585752720000',\n",
       " 'outputs-1585752730000',\n",
       " 'outputs-1585752740000',\n",
       " 'outputs-1585752750000',\n",
       " 'outputs-1585752760000',\n",
       " 'outputs-1585752770000',\n",
       " 'outputs-1585752780000',\n",
       " 'outputs-1585752790000',\n",
       " 'outputs-1585752800000',\n",
       " 'outputs-1585752810000',\n",
       " 'outputs-1585752820000',\n",
       " 'outputs-1585752830000',\n",
       " 'outputs-1585752840000',\n",
       " 'outputs-1585752850000',\n",
       " 'outputs-1585752860000',\n",
       " 'outputs-1585752870000',\n",
       " 'outputs-1585752880000',\n",
       " 'outputs-1585752890000',\n",
       " 'outputs-1585752900000',\n",
       " 'outputs-1585752910000',\n",
       " 'outputs-1585752920000',\n",
       " 'outputs-1585752930000',\n",
       " 'outputs-1585752940000',\n",
       " 'outputs-1585752950000',\n",
       " 'outputs-1585752960000',\n",
       " 'outputs-1585752970000',\n",
       " 'outputs-1585752980000',\n",
       " 'outputs-1585752990000',\n",
       " 'outputs-1585753000000',\n",
       " 'outputs-1585753010000',\n",
       " 'outputs-1585753020000',\n",
       " 'outputs-1585753030000',\n",
       " 'outputs-1585753040000',\n",
       " 'outputs-1585753050000',\n",
       " 'outputs-1585753060000',\n",
       " 'outputs-1585753070000',\n",
       " 'outputs-1585753080000',\n",
       " 'outputs-1585753090000',\n",
       " 'outputs-1585753100000',\n",
       " 'outputs-1585753110000',\n",
       " 'outputs-1585753120000',\n",
       " 'outputs-1585753130000',\n",
       " 'outputs-1585753140000',\n",
       " 'outputs-1585753150000',\n",
       " 'outputs-1585753160000',\n",
       " 'outputs-1585753170000',\n",
       " 'outputs-1585753180000',\n",
       " 'outputs-1585753190000',\n",
       " 'outputs-1585753200000',\n",
       " 'outputs-1585753210000',\n",
       " 'outputs-1585753220000',\n",
       " 'outputs-1585753230000',\n",
       " 'outputs-1585753240000',\n",
       " 'outputs-1585753250000',\n",
       " 'outputs-1585753260000',\n",
       " 'outputs-1585753270000',\n",
       " 'outputs-1585753280000',\n",
       " 'outputs-1585753290000',\n",
       " 'outputs-1585753300000',\n",
       " 'outputs-1585753310000',\n",
       " 'outputs-1585753320000',\n",
       " 'outputs-1585753330000',\n",
       " 'outputs-1585753340000',\n",
       " 'outputs-1585753350000',\n",
       " 'outputs-1585753360000',\n",
       " 'outputs-1585753370000',\n",
       " 'outputs-1585753380000',\n",
       " 'outputs-1585753390000',\n",
       " 'outputs-1585753400000',\n",
       " 'outputs-1585753410000',\n",
       " 'outputs-1585753420000',\n",
       " 'outputs-1585753430000',\n",
       " 'outputs-1585753440000',\n",
       " 'outputs-1585753450000',\n",
       " 'outputs-1585753460000',\n",
       " 'outputs-1585753470000',\n",
       " 'outputs-1585753480000',\n",
       " 'outputs-1585753490000',\n",
       " 'outputs-1585753500000',\n",
       " 'outputs-1585753510000',\n",
       " 'outputs-1585753520000',\n",
       " 'outputs-1585753530000',\n",
       " 'outputs-1585753540000',\n",
       " 'outputs-1585753550000',\n",
       " 'outputs-1585753560000',\n",
       " 'outputs-1585753570000',\n",
       " 'outputs-1585753580000',\n",
       " 'outputs-1585753590000',\n",
       " 'outputs-1585753600000',\n",
       " 'outputs-1585753610000',\n",
       " 'outputs-1585753620000',\n",
       " 'outputs-1585753630000',\n",
       " 'outputs-1585753640000',\n",
       " 'outputs-1585753650000',\n",
       " 'outputs-1585753660000',\n",
       " 'outputs-1585753670000',\n",
       " 'outputs-1585753680000',\n",
       " 'outputs-1585753690000',\n",
       " 'outputs-1585753700000',\n",
       " 'outputs-1585753710000',\n",
       " 'outputs-1585753720000',\n",
       " 'outputs-1585753730000',\n",
       " 'outputs-1585753740000',\n",
       " 'outputs-1585753750000',\n",
       " 'outputs-1585753760000',\n",
       " 'outputs-1585753770000',\n",
       " 'outputs-1585753780000',\n",
       " 'outputs-1585753790000',\n",
       " 'outputs-1585753800000',\n",
       " 'outputs-1585753810000',\n",
       " 'outputs-1585753820000',\n",
       " 'outputs-1585753830000',\n",
       " 'outputs-1585753840000',\n",
       " 'outputs-1585753850000',\n",
       " 'outputs-1585753860000',\n",
       " 'outputs-1585753870000',\n",
       " 'outputs-1585753880000',\n",
       " 'outputs-1585753890000',\n",
       " 'outputs-1585753900000',\n",
       " 'outputs-1585753910000',\n",
       " 'outputs-1585753920000',\n",
       " 'outputs-1585753930000',\n",
       " 'outputs-1585753940000',\n",
       " 'outputs-1585753950000',\n",
       " 'outputs-1585753960000',\n",
       " 'outputs-1585753970000',\n",
       " 'outputs-1585753980000',\n",
       " 'outputs-1585753990000',\n",
       " 'outputs-1585754000000',\n",
       " 'outputs-1585754010000',\n",
       " 'outputs-1585754020000',\n",
       " 'outputs-1585754030000',\n",
       " 'outputs-1585754040000',\n",
       " 'outputs-1585754050000',\n",
       " 'outputs-1585754060000',\n",
       " 'outputs-1585754070000',\n",
       " 'outputs-1585754080000',\n",
       " 'outputs-1585754090000',\n",
       " 'outputs-1585754100000',\n",
       " 'outputs-1585754110000',\n",
       " 'outputs-1585754120000',\n",
       " 'outputs-1585754130000',\n",
       " 'outputs-1585754140000',\n",
       " 'outputs-1585754150000',\n",
       " 'outputs-1585754160000',\n",
       " 'outputs-1585754170000',\n",
       " 'outputs-1585754180000',\n",
       " 'outputs-1585754190000',\n",
       " 'outputs-1585754200000',\n",
       " 'outputs-1585754210000',\n",
       " 'outputs-1585754220000',\n",
       " 'outputs-1585754230000',\n",
       " 'outputs-1585754240000',\n",
       " 'outputs-1585754250000',\n",
       " 'outputs-1585754260000',\n",
       " 'outputs-1585754270000',\n",
       " 'outputs-1585754280000',\n",
       " 'outputs-1585754290000',\n",
       " 'outputs-1585754300000',\n",
       " 'outputs-1585754310000',\n",
       " 'outputs-1585754320000',\n",
       " 'outputs-1585754330000',\n",
       " 'outputs-1585754340000',\n",
       " 'outputs-1585754350000',\n",
       " 'outputs-1585754360000',\n",
       " 'outputs-1585754370000',\n",
       " 'outputs-1585754380000',\n",
       " 'outputs-1585754390000',\n",
       " 'outputs-1585754400000',\n",
       " 'outputs-1585754410000',\n",
       " 'outputs-1585754420000',\n",
       " 'outputs-1585754430000',\n",
       " 'outputs-1585754440000',\n",
       " 'outputs-1585754450000',\n",
       " 'outputs-1585754460000',\n",
       " 'outputs-1585754470000',\n",
       " 'outputs-1585754480000',\n",
       " 'outputs-1585754490000',\n",
       " 'outputs-1585754500000',\n",
       " 'outputs-1585754510000',\n",
       " 'outputs-1585754520000',\n",
       " 'outputs-1585754530000',\n",
       " 'outputs-1585754540000',\n",
       " 'outputs-1585754550000',\n",
       " 'outputs-1585754560000',\n",
       " 'outputs-1585754570000',\n",
       " 'outputs-1585754580000',\n",
       " 'outputs-1585754590000',\n",
       " 'outputs-1585754600000',\n",
       " 'outputs-1585754610000',\n",
       " 'outputs-1585754620000',\n",
       " 'outputs-1585754630000',\n",
       " 'outputs-1585754640000',\n",
       " 'outputs-1585754650000',\n",
       " 'outputs-1585754660000',\n",
       " 'outputs-1585754670000',\n",
       " 'outputs-1585754680000',\n",
       " 'outputs-1585754690000',\n",
       " 'outputs-1585754700000',\n",
       " 'outputs-1585754710000',\n",
       " 'outputs-1585754720000',\n",
       " 'outputs-1585754730000',\n",
       " 'outputs-1585754740000',\n",
       " 'outputs-1585754750000',\n",
       " 'outputs-1585754760000',\n",
       " 'outputs-1585754770000',\n",
       " 'outputs-1585754780000',\n",
       " 'outputs-1585754790000',\n",
       " 'outputs-1585754800000',\n",
       " 'outputs-1585754810000',\n",
       " 'outputs-1585754820000',\n",
       " 'outputs-1585754830000',\n",
       " 'outputs-1585754840000',\n",
       " 'outputs-1585754850000',\n",
       " 'outputs-1585754860000',\n",
       " 'outputs-1585754870000',\n",
       " 'outputs-1585754880000',\n",
       " 'outputs-1585754890000',\n",
       " 'outputs-1585754900000',\n",
       " 'outputs-1585754910000',\n",
       " 'outputs-1585754920000',\n",
       " 'outputs-1585754930000',\n",
       " 'outputs-1585754940000',\n",
       " 'outputs-1585754950000',\n",
       " 'outputs-1585754960000',\n",
       " 'outputs-1585754970000',\n",
       " 'outputs-1585754980000',\n",
       " 'outputs-1585754990000',\n",
       " 'outputs-1585755000000',\n",
       " 'outputs-1585755010000',\n",
       " 'outputs-1585755020000',\n",
       " 'outputs-1585755030000',\n",
       " 'outputs-1585755040000',\n",
       " 'outputs-1585755050000',\n",
       " 'outputs-1585755060000',\n",
       " 'outputs-1585755070000',\n",
       " 'outputs-1585755080000',\n",
       " 'outputs-1585755090000',\n",
       " 'outputs-1585755100000',\n",
       " 'outputs-1585755110000',\n",
       " 'outputs-1585755120000',\n",
       " 'outputs-1585755130000',\n",
       " 'outputs-1585755140000',\n",
       " 'outputs-1585755150000',\n",
       " 'outputs-1585755160000',\n",
       " 'outputs-1585755170000',\n",
       " 'outputs-1585755180000',\n",
       " 'outputs-1585755190000',\n",
       " 'outputs-1585755200000',\n",
       " 'outputs-1585755210000',\n",
       " 'outputs-1585755220000',\n",
       " 'outputs-1585755230000',\n",
       " 'outputs-1585755240000',\n",
       " 'outputs-1585755250000',\n",
       " 'outputs-1585755260000',\n",
       " 'outputs-1585755270000',\n",
       " 'outputs-1585755280000',\n",
       " 'outputs-1585755290000',\n",
       " 'outputs-1585755300000',\n",
       " 'outputs-1585755310000',\n",
       " 'outputs-1585755320000',\n",
       " 'outputs-1585755330000',\n",
       " 'outputs-1585755340000',\n",
       " 'outputs-1585755350000',\n",
       " 'outputs-1585755360000',\n",
       " 'outputs-1585755370000',\n",
       " 'outputs-1585755380000',\n",
       " 'outputs-1585755390000',\n",
       " 'outputs-1585755400000',\n",
       " 'outputs-1585755410000',\n",
       " 'outputs-1585755420000',\n",
       " 'outputs-1585755430000',\n",
       " 'outputs-1585755440000',\n",
       " 'outputs-1585755450000',\n",
       " 'outputs-1585755460000',\n",
       " 'outputs-1585755470000',\n",
       " 'outputs-1585755480000',\n",
       " 'outputs-1585755490000',\n",
       " 'outputs-1585755500000',\n",
       " 'outputs-1585755510000',\n",
       " 'outputs-1585755520000',\n",
       " 'outputs-1585755530000',\n",
       " 'outputs-1585755540000',\n",
       " 'outputs-1585755550000',\n",
       " 'outputs-1585755560000',\n",
       " 'outputs-1585755570000',\n",
       " 'outputs-1585755580000',\n",
       " 'outputs-1585755590000',\n",
       " 'outputs-1585755600000',\n",
       " 'outputs-1585755610000',\n",
       " 'outputs-1585755620000',\n",
       " 'outputs-1585755630000',\n",
       " 'outputs-1585755640000',\n",
       " 'outputs-1585755650000',\n",
       " 'outputs-1585755660000',\n",
       " 'outputs-1585755670000',\n",
       " 'outputs-1585755680000',\n",
       " 'outputs-1585755690000',\n",
       " 'outputs-1585755700000',\n",
       " 'outputs-1585755710000',\n",
       " 'outputs-1585755720000',\n",
       " 'outputs-1585755730000',\n",
       " 'outputs-1585755740000',\n",
       " 'outputs-1585755750000',\n",
       " 'outputs-1585755760000',\n",
       " 'outputs-1585755770000',\n",
       " 'outputs-1585755780000',\n",
       " 'outputs-1585755790000',\n",
       " 'outputs-1585755800000',\n",
       " 'outputs-1585755810000',\n",
       " 'outputs-1585755820000',\n",
       " 'outputs-1585755830000',\n",
       " 'outputs-1585755840000',\n",
       " 'outputs-1585755850000',\n",
       " 'outputs-1585755860000',\n",
       " 'outputs-1585755870000',\n",
       " 'outputs-1585755880000',\n",
       " 'outputs-1585755890000',\n",
       " 'outputs-1585755900000',\n",
       " 'outputs-1585755910000',\n",
       " 'outputs-1585755920000',\n",
       " 'outputs-1585755930000',\n",
       " 'outputs-1585755940000',\n",
       " 'outputs-1585755950000',\n",
       " 'outputs-1585755960000',\n",
       " 'outputs-1585755970000',\n",
       " 'outputs-1585755980000',\n",
       " 'outputs-1585755990000',\n",
       " 'outputs-1585756000000',\n",
       " 'outputs-1585756010000',\n",
       " 'outputs-1585756020000',\n",
       " 'outputs-1585756030000',\n",
       " 'outputs-1585756040000',\n",
       " 'outputs-1585756050000',\n",
       " 'outputs-1585756060000',\n",
       " 'outputs-1585756070000',\n",
       " 'outputs-1585756080000',\n",
       " 'outputs-1585756090000',\n",
       " 'outputs-1585756100000',\n",
       " 'outputs-1585756110000',\n",
       " 'outputs-1585756120000',\n",
       " 'outputs-1585756130000',\n",
       " 'outputs-1585756140000',\n",
       " 'outputs-1585756150000',\n",
       " 'outputs-1585756160000',\n",
       " 'outputs-1585756170000',\n",
       " 'outputs-1585756180000',\n",
       " 'outputs-1585756190000',\n",
       " 'outputs-1585756200000',\n",
       " 'outputs-1585756210000',\n",
       " 'outputs-1585756220000',\n",
       " 'outputs-1585756230000',\n",
       " 'outputs-1585756240000',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = os.listdir('C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\streaming_1a')\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- comment: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- text_new: string (nullable = true)\n",
      " |-- text_old: string (nullable = true)\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load('C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\test\\\\*')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diff(old, new):\n",
    "    return '\\n'.join([ l for l in unified_diff(old.split('\\n'), new.split('\\n')) if l.startswith('+') or l.startswith('-') ])\n",
    "\n",
    "udfMake_Diff = udf(make_diff, StringType())\n",
    "\n",
    "def add_diff(df):\n",
    "    df.withColumn(\"diff\", udfMake_Diff(\"text_old\", \"text_new\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffColTransformer(Transformer, ml.util.DefaultParamsWritable, ml.util.DefaultParamsReadable):\n",
    "    '''Custom transformer to get udf makeDiff into pipeline.'''\n",
    "    def __init__(self):\n",
    "        super(DiffColTransformer, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        df = df.withColumn(\"diff\", udfMake_Diff(\"text_old\", \"text_new\"))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = DiffColTransformer()\n",
    "tokenizer = RegexTokenizer(inputCol=\"diff\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "idf = IDF(inputCol=\"vectors\", outputCol=\"features\")\n",
    "label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"target\")\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"target\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [dct, tokenizer, remover, cv, idf, label_indexer, nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(stages=stages).fit(dataset = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\SparkStream'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2609.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:441)\r\n\tat org.apache.spark.ml.classification.NaiveBayesModel$NaiveBayesModelWriter.saveImpl(NaiveBayes.scala:389)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:180)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 80.0 failed 1 times, most recent failure: Lost task 0.0 in stage 80.0 (TID 203, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\\tmp\\nb\\metadata\\_temporary\\0\\_temporary\\attempt_20200518142432_0408_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 42 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\\tmp\\nb\\metadata\\_temporary\\0\\_temporary\\attempt_20200518142432_0408_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-2b84409a89b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tmp/nb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2609.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:441)\r\n\tat org.apache.spark.ml.classification.NaiveBayesModel$NaiveBayesModelWriter.saveImpl(NaiveBayes.scala:389)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:180)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 80.0 failed 1 times, most recent failure: Lost task 0.0 in stage 80.0 (TID 203, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\\tmp\\nb\\metadata\\_temporary\\0\\_temporary\\attempt_20200518142432_0408_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 42 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\\tmp\\nb\\metadata\\_temporary\\0\\_temporary\\attempt_20200518142432_0408_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "model.stages[6].save(\"tmp/nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2668.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 204, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\\metadata\\_temporary\\0\\_temporary\\attempt_20200518151020_0412_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 41 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\\metadata\\_temporary\\0\\_temporary\\attempt_20200518151020_0412_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-2eb26ef77cd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshouldOverwrite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handleOverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveImpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msaveImpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36msaveImpl\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mstages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mPipelineSharedReadWrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidateStages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mPipelineSharedReadWrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveImpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36msaveImpl\u001b[1;34m(instance, stages, sc, path)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[0mstageUids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[0mjsonParams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'stageUids'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstageUids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'language'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Python'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m         \u001b[0mDefaultParamsWriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjsonParams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[0mstagesDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"stages\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msaveMetadata\u001b[1;34m(instance, path, sc, extraMetadata, paramMap)\u001b[0m\n\u001b[0;32m    454\u001b[0m                                                                  \u001b[0mextraMetadata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m                                                                  paramMap)\n\u001b[1;32m--> 456\u001b[1;33m         \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetadataJson\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadataPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1570\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2668.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 204, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\\metadata\\_temporary\\0\\_temporary\\attempt_20200518151020_0412_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 41 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\\metadata\\_temporary\\0\\_temporary\\attempt_20200518151020_0412_m_000000_0\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\r\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "model.save(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
