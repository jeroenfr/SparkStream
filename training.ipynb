{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\SparkStream\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from difflib import unified_diff\n",
    "from pyspark import ml\n",
    "import numpy as np\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diff(old, new):\n",
    "    return '\\n'.join([ l for l in unified_diff(old.split('\\n'), new.split('\\n')) if l.startswith('+') or l.startswith('-') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to extract the added pieces of text in the document.'''\n",
    "def get_added_text(diff):\n",
    "    edits = []\n",
    "    for text in diff.split('\\n'):\n",
    "        if text.startswith(\"+\"):\n",
    "            edits.append(text)\n",
    "    #print(edits)        \n",
    "    return edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://GHUM-L-E28RMVT2.luna.kuleuven.be:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c4e5ab2a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('example').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.format(\"json\").load(cwd+\"\\Assignment3\\\\streaming_1a\\\\outputs-*\\\\part-*\")\n",
    "\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\streaming_1a')\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DirEntry 'naive_bayes'>,\n",
       " <DirEntry 'outputs-1585746250000'>,\n",
       " <DirEntry 'outputs-1585746260000'>,\n",
       " <DirEntry 'outputs-1585746270000'>,\n",
       " <DirEntry 'outputs-1585746280000'>,\n",
       " <DirEntry 'outputs-1585746290000'>,\n",
       " <DirEntry 'outputs-1585746300000'>,\n",
       " <DirEntry 'outputs-1585746310000'>,\n",
       " <DirEntry 'outputs-1585746320000'>,\n",
       " <DirEntry 'outputs-1585746330000'>,\n",
       " <DirEntry 'outputs-1585746340000'>,\n",
       " <DirEntry 'outputs-1585746350000'>,\n",
       " <DirEntry 'outputs-1585746360000'>,\n",
       " <DirEntry 'outputs-1585746370000'>,\n",
       " <DirEntry 'outputs-1585746380000'>,\n",
       " <DirEntry 'outputs-1585746390000'>,\n",
       " <DirEntry 'outputs-1585746400000'>,\n",
       " <DirEntry 'outputs-1585746410000'>,\n",
       " <DirEntry 'outputs-1585746420000'>,\n",
       " <DirEntry 'outputs-1585746430000'>,\n",
       " <DirEntry 'outputs-1585746440000'>,\n",
       " <DirEntry 'outputs-1585746450000'>,\n",
       " <DirEntry 'outputs-1585746460000'>,\n",
       " <DirEntry 'outputs-1585746470000'>,\n",
       " <DirEntry 'outputs-1585746480000'>,\n",
       " <DirEntry 'outputs-1585746490000'>,\n",
       " <DirEntry 'outputs-1585746500000'>,\n",
       " <DirEntry 'outputs-1585746510000'>,\n",
       " <DirEntry 'outputs-1585746520000'>,\n",
       " <DirEntry 'outputs-1585746530000'>,\n",
       " <DirEntry 'outputs-1585746540000'>,\n",
       " <DirEntry 'outputs-1585746550000'>,\n",
       " <DirEntry 'outputs-1585746560000'>,\n",
       " <DirEntry 'outputs-1585746570000'>,\n",
       " <DirEntry 'outputs-1585746580000'>,\n",
       " <DirEntry 'outputs-1585746590000'>,\n",
       " <DirEntry 'outputs-1585746600000'>,\n",
       " <DirEntry 'outputs-1585746610000'>,\n",
       " <DirEntry 'outputs-1585746620000'>,\n",
       " <DirEntry 'outputs-1585746630000'>,\n",
       " <DirEntry 'outputs-1585746640000'>,\n",
       " <DirEntry 'outputs-1585746650000'>,\n",
       " <DirEntry 'outputs-1585746660000'>,\n",
       " <DirEntry 'outputs-1585746670000'>,\n",
       " <DirEntry 'outputs-1585746680000'>,\n",
       " <DirEntry 'outputs-1585746690000'>,\n",
       " <DirEntry 'outputs-1585746700000'>,\n",
       " <DirEntry 'outputs-1585746710000'>,\n",
       " <DirEntry 'outputs-1585746720000'>,\n",
       " <DirEntry 'outputs-1585746730000'>,\n",
       " <DirEntry 'outputs-1585746740000'>,\n",
       " <DirEntry 'outputs-1585746750000'>,\n",
       " <DirEntry 'outputs-1585746760000'>,\n",
       " <DirEntry 'outputs-1585746770000'>,\n",
       " <DirEntry 'outputs-1585746780000'>,\n",
       " <DirEntry 'outputs-1585746790000'>,\n",
       " <DirEntry 'outputs-1585746800000'>,\n",
       " <DirEntry 'outputs-1585746810000'>,\n",
       " <DirEntry 'outputs-1585746820000'>,\n",
       " <DirEntry 'outputs-1585746830000'>,\n",
       " <DirEntry 'outputs-1585746840000'>,\n",
       " <DirEntry 'outputs-1585746850000'>,\n",
       " <DirEntry 'outputs-1585746860000'>,\n",
       " <DirEntry 'outputs-1585746870000'>,\n",
       " <DirEntry 'outputs-1585746880000'>,\n",
       " <DirEntry 'outputs-1585746890000'>,\n",
       " <DirEntry 'outputs-1585746900000'>,\n",
       " <DirEntry 'outputs-1585746910000'>,\n",
       " <DirEntry 'outputs-1585746920000'>,\n",
       " <DirEntry 'outputs-1585746930000'>,\n",
       " <DirEntry 'outputs-1585746940000'>,\n",
       " <DirEntry 'outputs-1585746950000'>,\n",
       " <DirEntry 'outputs-1585746960000'>,\n",
       " <DirEntry 'outputs-1585746970000'>,\n",
       " <DirEntry 'outputs-1585746980000'>,\n",
       " <DirEntry 'outputs-1585746990000'>,\n",
       " <DirEntry 'outputs-1585747000000'>,\n",
       " <DirEntry 'outputs-1585747010000'>,\n",
       " <DirEntry 'outputs-1585747020000'>,\n",
       " <DirEntry 'outputs-1585747030000'>,\n",
       " <DirEntry 'outputs-1585747040000'>,\n",
       " <DirEntry 'outputs-1585747050000'>,\n",
       " <DirEntry 'outputs-1585747060000'>,\n",
       " <DirEntry 'outputs-1585747070000'>,\n",
       " <DirEntry 'outputs-1585747080000'>,\n",
       " <DirEntry 'outputs-1585747090000'>,\n",
       " <DirEntry 'outputs-1585747100000'>,\n",
       " <DirEntry 'outputs-1585747110000'>,\n",
       " <DirEntry 'outputs-1585747120000'>,\n",
       " <DirEntry 'outputs-1585747130000'>,\n",
       " <DirEntry 'outputs-1585747140000'>,\n",
       " <DirEntry 'outputs-1585747150000'>,\n",
       " <DirEntry 'outputs-1585747160000'>,\n",
       " <DirEntry 'outputs-1585747170000'>,\n",
       " <DirEntry 'outputs-1585747180000'>,\n",
       " <DirEntry 'outputs-1585747190000'>,\n",
       " <DirEntry 'outputs-1585747200000'>,\n",
       " <DirEntry 'outputs-1585747210000'>,\n",
       " <DirEntry 'outputs-1585747220000'>,\n",
       " <DirEntry 'outputs-1585747230000'>,\n",
       " <DirEntry 'outputs-1585747240000'>,\n",
       " <DirEntry 'outputs-1585747250000'>,\n",
       " <DirEntry 'outputs-1585747260000'>,\n",
       " <DirEntry 'outputs-1585747270000'>,\n",
       " <DirEntry 'outputs-1585747280000'>,\n",
       " <DirEntry 'outputs-1585747290000'>,\n",
       " <DirEntry 'outputs-1585747300000'>,\n",
       " <DirEntry 'outputs-1585747310000'>,\n",
       " <DirEntry 'outputs-1585747320000'>,\n",
       " <DirEntry 'outputs-1585747330000'>,\n",
       " <DirEntry 'outputs-1585747340000'>,\n",
       " <DirEntry 'outputs-1585747350000'>,\n",
       " <DirEntry 'outputs-1585747360000'>,\n",
       " <DirEntry 'outputs-1585747370000'>,\n",
       " <DirEntry 'outputs-1585747380000'>,\n",
       " <DirEntry 'outputs-1585747390000'>,\n",
       " <DirEntry 'outputs-1585747400000'>,\n",
       " <DirEntry 'outputs-1585747410000'>,\n",
       " <DirEntry 'outputs-1585747420000'>,\n",
       " <DirEntry 'outputs-1585747430000'>,\n",
       " <DirEntry 'outputs-1585747440000'>,\n",
       " <DirEntry 'outputs-1585747450000'>,\n",
       " <DirEntry 'outputs-1585747460000'>,\n",
       " <DirEntry 'outputs-1585747470000'>,\n",
       " <DirEntry 'outputs-1585747480000'>,\n",
       " <DirEntry 'outputs-1585747490000'>,\n",
       " <DirEntry 'outputs-1585747500000'>,\n",
       " <DirEntry 'outputs-1585747510000'>,\n",
       " <DirEntry 'outputs-1585747520000'>,\n",
       " <DirEntry 'outputs-1585747530000'>,\n",
       " <DirEntry 'outputs-1585747540000'>,\n",
       " <DirEntry 'outputs-1585747550000'>,\n",
       " <DirEntry 'outputs-1585747560000'>,\n",
       " <DirEntry 'outputs-1585747570000'>,\n",
       " <DirEntry 'outputs-1585747580000'>,\n",
       " <DirEntry 'outputs-1585747590000'>,\n",
       " <DirEntry 'outputs-1585747600000'>,\n",
       " <DirEntry 'outputs-1585747610000'>,\n",
       " <DirEntry 'outputs-1585747620000'>,\n",
       " <DirEntry 'outputs-1585747630000'>,\n",
       " <DirEntry 'outputs-1585747640000'>,\n",
       " <DirEntry 'outputs-1585747650000'>,\n",
       " <DirEntry 'outputs-1585747660000'>,\n",
       " <DirEntry 'outputs-1585747670000'>,\n",
       " <DirEntry 'outputs-1585747680000'>,\n",
       " <DirEntry 'outputs-1585747690000'>,\n",
       " <DirEntry 'outputs-1585747700000'>,\n",
       " <DirEntry 'outputs-1585747710000'>,\n",
       " <DirEntry 'outputs-1585747720000'>,\n",
       " <DirEntry 'outputs-1585747730000'>,\n",
       " <DirEntry 'outputs-1585747740000'>,\n",
       " <DirEntry 'outputs-1585747750000'>,\n",
       " <DirEntry 'outputs-1585747760000'>,\n",
       " <DirEntry 'outputs-1585747770000'>,\n",
       " <DirEntry 'outputs-1585747780000'>,\n",
       " <DirEntry 'outputs-1585747790000'>,\n",
       " <DirEntry 'outputs-1585747800000'>,\n",
       " <DirEntry 'outputs-1585747810000'>,\n",
       " <DirEntry 'outputs-1585747820000'>,\n",
       " <DirEntry 'outputs-1585747830000'>,\n",
       " <DirEntry 'outputs-1585747840000'>,\n",
       " <DirEntry 'outputs-1585747850000'>,\n",
       " <DirEntry 'outputs-1585747860000'>,\n",
       " <DirEntry 'outputs-1585747870000'>,\n",
       " <DirEntry 'outputs-1585747880000'>,\n",
       " <DirEntry 'outputs-1585747890000'>,\n",
       " <DirEntry 'outputs-1585747900000'>,\n",
       " <DirEntry 'outputs-1585747910000'>,\n",
       " <DirEntry 'outputs-1585747920000'>,\n",
       " <DirEntry 'outputs-1585747930000'>,\n",
       " <DirEntry 'outputs-1585747940000'>,\n",
       " <DirEntry 'outputs-1585747950000'>,\n",
       " <DirEntry 'outputs-1585747960000'>,\n",
       " <DirEntry 'outputs-1585747970000'>,\n",
       " <DirEntry 'outputs-1585747980000'>,\n",
       " <DirEntry 'outputs-1585747990000'>,\n",
       " <DirEntry 'outputs-1585748000000'>,\n",
       " <DirEntry 'outputs-1585748010000'>,\n",
       " <DirEntry 'outputs-1585748020000'>,\n",
       " <DirEntry 'outputs-1585748030000'>,\n",
       " <DirEntry 'outputs-1585748040000'>,\n",
       " <DirEntry 'outputs-1585748050000'>,\n",
       " <DirEntry 'outputs-1585748060000'>,\n",
       " <DirEntry 'outputs-1585748070000'>,\n",
       " <DirEntry 'outputs-1585748080000'>,\n",
       " <DirEntry 'outputs-1585748090000'>,\n",
       " <DirEntry 'outputs-1585748100000'>,\n",
       " <DirEntry 'outputs-1585748110000'>,\n",
       " <DirEntry 'outputs-1585748120000'>,\n",
       " <DirEntry 'outputs-1585748130000'>,\n",
       " <DirEntry 'outputs-1585748140000'>,\n",
       " <DirEntry 'outputs-1585748150000'>,\n",
       " <DirEntry 'outputs-1585748160000'>,\n",
       " <DirEntry 'outputs-1585748170000'>,\n",
       " <DirEntry 'outputs-1585748180000'>,\n",
       " <DirEntry 'outputs-1585748190000'>,\n",
       " <DirEntry 'outputs-1585748200000'>,\n",
       " <DirEntry 'outputs-1585748210000'>,\n",
       " <DirEntry 'outputs-1585748220000'>,\n",
       " <DirEntry 'outputs-1585748230000'>,\n",
       " <DirEntry 'outputs-1585748240000'>,\n",
       " <DirEntry 'outputs-1585748250000'>,\n",
       " <DirEntry 'outputs-1585748260000'>,\n",
       " <DirEntry 'outputs-1585748270000'>,\n",
       " <DirEntry 'outputs-1585748280000'>,\n",
       " <DirEntry 'outputs-1585748290000'>,\n",
       " <DirEntry 'outputs-1585748300000'>,\n",
       " <DirEntry 'outputs-1585748310000'>,\n",
       " <DirEntry 'outputs-1585748320000'>,\n",
       " <DirEntry 'outputs-1585748330000'>,\n",
       " <DirEntry 'outputs-1585748340000'>,\n",
       " <DirEntry 'outputs-1585748350000'>,\n",
       " <DirEntry 'outputs-1585748360000'>,\n",
       " <DirEntry 'outputs-1585748370000'>,\n",
       " <DirEntry 'outputs-1585748380000'>,\n",
       " <DirEntry 'outputs-1585748390000'>,\n",
       " <DirEntry 'outputs-1585748400000'>,\n",
       " <DirEntry 'outputs-1585748410000'>,\n",
       " <DirEntry 'outputs-1585748420000'>,\n",
       " <DirEntry 'outputs-1585748430000'>,\n",
       " <DirEntry 'outputs-1585748440000'>,\n",
       " <DirEntry 'outputs-1585748450000'>,\n",
       " <DirEntry 'outputs-1585748460000'>,\n",
       " <DirEntry 'outputs-1585748470000'>,\n",
       " <DirEntry 'outputs-1585748480000'>,\n",
       " <DirEntry 'outputs-1585748490000'>,\n",
       " <DirEntry 'outputs-1585748500000'>,\n",
       " <DirEntry 'outputs-1585748510000'>,\n",
       " <DirEntry 'outputs-1585748520000'>,\n",
       " <DirEntry 'outputs-1585748530000'>,\n",
       " <DirEntry 'outputs-1585748540000'>,\n",
       " <DirEntry 'outputs-1585748550000'>,\n",
       " <DirEntry 'outputs-1585748560000'>,\n",
       " <DirEntry 'outputs-1585748570000'>,\n",
       " <DirEntry 'outputs-1585748580000'>,\n",
       " <DirEntry 'outputs-1585748590000'>,\n",
       " <DirEntry 'outputs-1585748600000'>,\n",
       " <DirEntry 'outputs-1585748610000'>,\n",
       " <DirEntry 'outputs-1585748620000'>,\n",
       " <DirEntry 'outputs-1585748630000'>,\n",
       " <DirEntry 'outputs-1585748640000'>,\n",
       " <DirEntry 'outputs-1585748650000'>,\n",
       " <DirEntry 'outputs-1585748660000'>,\n",
       " <DirEntry 'outputs-1585748670000'>,\n",
       " <DirEntry 'outputs-1585748680000'>,\n",
       " <DirEntry 'outputs-1585748690000'>,\n",
       " <DirEntry 'outputs-1585748700000'>,\n",
       " <DirEntry 'outputs-1585748710000'>,\n",
       " <DirEntry 'outputs-1585748720000'>,\n",
       " <DirEntry 'outputs-1585748730000'>,\n",
       " <DirEntry 'outputs-1585748740000'>,\n",
       " <DirEntry 'outputs-1585748750000'>,\n",
       " <DirEntry 'outputs-1585748760000'>,\n",
       " <DirEntry 'outputs-1585748770000'>,\n",
       " <DirEntry 'outputs-1585748780000'>,\n",
       " <DirEntry 'outputs-1585748790000'>,\n",
       " <DirEntry 'outputs-1585748800000'>,\n",
       " <DirEntry 'outputs-1585748810000'>,\n",
       " <DirEntry 'outputs-1585748820000'>,\n",
       " <DirEntry 'outputs-1585748830000'>,\n",
       " <DirEntry 'outputs-1585748840000'>,\n",
       " <DirEntry 'outputs-1585748850000'>,\n",
       " <DirEntry 'outputs-1585748860000'>,\n",
       " <DirEntry 'outputs-1585748870000'>,\n",
       " <DirEntry 'outputs-1585748880000'>,\n",
       " <DirEntry 'outputs-1585748890000'>,\n",
       " <DirEntry 'outputs-1585748900000'>,\n",
       " <DirEntry 'outputs-1585748910000'>,\n",
       " <DirEntry 'outputs-1585748920000'>,\n",
       " <DirEntry 'outputs-1585748930000'>,\n",
       " <DirEntry 'outputs-1585748940000'>,\n",
       " <DirEntry 'outputs-1585748950000'>,\n",
       " <DirEntry 'outputs-1585748960000'>,\n",
       " <DirEntry 'outputs-1585748970000'>,\n",
       " <DirEntry 'outputs-1585748980000'>,\n",
       " <DirEntry 'outputs-1585748990000'>,\n",
       " <DirEntry 'outputs-1585749000000'>,\n",
       " <DirEntry 'outputs-1585749010000'>,\n",
       " <DirEntry 'outputs-1585749020000'>,\n",
       " <DirEntry 'outputs-1585749030000'>,\n",
       " <DirEntry 'outputs-1585749040000'>,\n",
       " <DirEntry 'outputs-1585749050000'>,\n",
       " <DirEntry 'outputs-1585749060000'>,\n",
       " <DirEntry 'outputs-1585749070000'>,\n",
       " <DirEntry 'outputs-1585749080000'>,\n",
       " <DirEntry 'outputs-1585749090000'>,\n",
       " <DirEntry 'outputs-1585749100000'>,\n",
       " <DirEntry 'outputs-1585749110000'>,\n",
       " <DirEntry 'outputs-1585749120000'>,\n",
       " <DirEntry 'outputs-1585749130000'>,\n",
       " <DirEntry 'outputs-1585749140000'>,\n",
       " <DirEntry 'outputs-1585749150000'>,\n",
       " <DirEntry 'outputs-1585749160000'>,\n",
       " <DirEntry 'outputs-1585749170000'>,\n",
       " <DirEntry 'outputs-1585749180000'>,\n",
       " <DirEntry 'outputs-1585749190000'>,\n",
       " <DirEntry 'outputs-1585749200000'>,\n",
       " <DirEntry 'outputs-1585749210000'>,\n",
       " <DirEntry 'outputs-1585749220000'>,\n",
       " <DirEntry 'outputs-1585749230000'>,\n",
       " <DirEntry 'outputs-1585749240000'>,\n",
       " <DirEntry 'outputs-1585749250000'>,\n",
       " <DirEntry 'outputs-1585749260000'>,\n",
       " <DirEntry 'outputs-1585749270000'>,\n",
       " <DirEntry 'outputs-1585749280000'>,\n",
       " <DirEntry 'outputs-1585749290000'>,\n",
       " <DirEntry 'outputs-1585749300000'>,\n",
       " <DirEntry 'outputs-1585749310000'>,\n",
       " <DirEntry 'outputs-1585749320000'>,\n",
       " <DirEntry 'outputs-1585749330000'>,\n",
       " <DirEntry 'outputs-1585749340000'>,\n",
       " <DirEntry 'outputs-1585749350000'>,\n",
       " <DirEntry 'outputs-1585749360000'>,\n",
       " <DirEntry 'outputs-1585749370000'>,\n",
       " <DirEntry 'outputs-1585749380000'>,\n",
       " <DirEntry 'outputs-1585749390000'>,\n",
       " <DirEntry 'outputs-1585749400000'>,\n",
       " <DirEntry 'outputs-1585749410000'>,\n",
       " <DirEntry 'outputs-1585749420000'>,\n",
       " <DirEntry 'outputs-1585749430000'>,\n",
       " <DirEntry 'outputs-1585749440000'>,\n",
       " <DirEntry 'outputs-1585749450000'>,\n",
       " <DirEntry 'outputs-1585749460000'>,\n",
       " <DirEntry 'outputs-1585749470000'>,\n",
       " <DirEntry 'outputs-1585749480000'>,\n",
       " <DirEntry 'outputs-1585749490000'>,\n",
       " <DirEntry 'outputs-1585749500000'>,\n",
       " <DirEntry 'outputs-1585749510000'>,\n",
       " <DirEntry 'outputs-1585749520000'>,\n",
       " <DirEntry 'outputs-1585749530000'>,\n",
       " <DirEntry 'outputs-1585749540000'>,\n",
       " <DirEntry 'outputs-1585749550000'>,\n",
       " <DirEntry 'outputs-1585749560000'>,\n",
       " <DirEntry 'outputs-1585749570000'>,\n",
       " <DirEntry 'outputs-1585749580000'>,\n",
       " <DirEntry 'outputs-1585749590000'>,\n",
       " <DirEntry 'outputs-1585749600000'>,\n",
       " <DirEntry 'outputs-1585749610000'>,\n",
       " <DirEntry 'outputs-1585749620000'>,\n",
       " <DirEntry 'outputs-1585749630000'>,\n",
       " <DirEntry 'outputs-1585749640000'>,\n",
       " <DirEntry 'outputs-1585749650000'>,\n",
       " <DirEntry 'outputs-1585749660000'>,\n",
       " <DirEntry 'outputs-1585749670000'>,\n",
       " <DirEntry 'outputs-1585749680000'>,\n",
       " <DirEntry 'outputs-1585749690000'>,\n",
       " <DirEntry 'outputs-1585749700000'>,\n",
       " <DirEntry 'outputs-1585749710000'>,\n",
       " <DirEntry 'outputs-1585749720000'>,\n",
       " <DirEntry 'outputs-1585749730000'>,\n",
       " <DirEntry 'outputs-1585749740000'>,\n",
       " <DirEntry 'outputs-1585749750000'>,\n",
       " <DirEntry 'outputs-1585749760000'>,\n",
       " <DirEntry 'outputs-1585749770000'>,\n",
       " <DirEntry 'outputs-1585749780000'>,\n",
       " <DirEntry 'outputs-1585749790000'>,\n",
       " <DirEntry 'outputs-1585749800000'>,\n",
       " <DirEntry 'outputs-1585749810000'>,\n",
       " <DirEntry 'outputs-1585749820000'>,\n",
       " <DirEntry 'outputs-1585749830000'>,\n",
       " <DirEntry 'outputs-1585749840000'>,\n",
       " <DirEntry 'outputs-1585749850000'>,\n",
       " <DirEntry 'outputs-1585749860000'>,\n",
       " <DirEntry 'outputs-1585749870000'>,\n",
       " <DirEntry 'outputs-1585749880000'>,\n",
       " <DirEntry 'outputs-1585749890000'>,\n",
       " <DirEntry 'outputs-1585749900000'>,\n",
       " <DirEntry 'outputs-1585749910000'>,\n",
       " <DirEntry 'outputs-1585749920000'>,\n",
       " <DirEntry 'outputs-1585749930000'>,\n",
       " <DirEntry 'outputs-1585749940000'>,\n",
       " <DirEntry 'outputs-1585749950000'>,\n",
       " <DirEntry 'outputs-1585749960000'>,\n",
       " <DirEntry 'outputs-1585749970000'>,\n",
       " <DirEntry 'outputs-1585749980000'>,\n",
       " <DirEntry 'outputs-1585749990000'>,\n",
       " <DirEntry 'outputs-1585750000000'>,\n",
       " <DirEntry 'outputs-1585750010000'>,\n",
       " <DirEntry 'outputs-1585750020000'>,\n",
       " <DirEntry 'outputs-1585750030000'>,\n",
       " <DirEntry 'outputs-1585750040000'>,\n",
       " <DirEntry 'outputs-1585750050000'>,\n",
       " <DirEntry 'outputs-1585750060000'>,\n",
       " <DirEntry 'outputs-1585750070000'>,\n",
       " <DirEntry 'outputs-1585750080000'>,\n",
       " <DirEntry 'outputs-1585750090000'>,\n",
       " <DirEntry 'outputs-1585750100000'>,\n",
       " <DirEntry 'outputs-1585750110000'>,\n",
       " <DirEntry 'outputs-1585750120000'>,\n",
       " <DirEntry 'outputs-1585750130000'>,\n",
       " <DirEntry 'outputs-1585750140000'>,\n",
       " <DirEntry 'outputs-1585750150000'>,\n",
       " <DirEntry 'outputs-1585750160000'>,\n",
       " <DirEntry 'outputs-1585750170000'>,\n",
       " <DirEntry 'outputs-1585750180000'>,\n",
       " <DirEntry 'outputs-1585750190000'>,\n",
       " <DirEntry 'outputs-1585750200000'>,\n",
       " <DirEntry 'outputs-1585750210000'>,\n",
       " <DirEntry 'outputs-1585750220000'>,\n",
       " <DirEntry 'outputs-1585750230000'>,\n",
       " <DirEntry 'outputs-1585750240000'>,\n",
       " <DirEntry 'outputs-1585750250000'>,\n",
       " <DirEntry 'outputs-1585750260000'>,\n",
       " <DirEntry 'outputs-1585750270000'>,\n",
       " <DirEntry 'outputs-1585750280000'>,\n",
       " <DirEntry 'outputs-1585750290000'>,\n",
       " <DirEntry 'outputs-1585750300000'>,\n",
       " <DirEntry 'outputs-1585750310000'>,\n",
       " <DirEntry 'outputs-1585750320000'>,\n",
       " <DirEntry 'outputs-1585750330000'>,\n",
       " <DirEntry 'outputs-1585750340000'>,\n",
       " <DirEntry 'outputs-1585750350000'>,\n",
       " <DirEntry 'outputs-1585750360000'>,\n",
       " <DirEntry 'outputs-1585750370000'>,\n",
       " <DirEntry 'outputs-1585750380000'>,\n",
       " <DirEntry 'outputs-1585750390000'>,\n",
       " <DirEntry 'outputs-1585750400000'>,\n",
       " <DirEntry 'outputs-1585750410000'>,\n",
       " <DirEntry 'outputs-1585750420000'>,\n",
       " <DirEntry 'outputs-1585750430000'>,\n",
       " <DirEntry 'outputs-1585750440000'>,\n",
       " <DirEntry 'outputs-1585750450000'>,\n",
       " <DirEntry 'outputs-1585750460000'>,\n",
       " <DirEntry 'outputs-1585750470000'>,\n",
       " <DirEntry 'outputs-1585750480000'>,\n",
       " <DirEntry 'outputs-1585750490000'>,\n",
       " <DirEntry 'outputs-1585750500000'>,\n",
       " <DirEntry 'outputs-1585750510000'>,\n",
       " <DirEntry 'outputs-1585750520000'>,\n",
       " <DirEntry 'outputs-1585750530000'>,\n",
       " <DirEntry 'outputs-1585750540000'>,\n",
       " <DirEntry 'outputs-1585750550000'>,\n",
       " <DirEntry 'outputs-1585750560000'>,\n",
       " <DirEntry 'outputs-1585750570000'>,\n",
       " <DirEntry 'outputs-1585750580000'>,\n",
       " <DirEntry 'outputs-1585750590000'>,\n",
       " <DirEntry 'outputs-1585750600000'>,\n",
       " <DirEntry 'outputs-1585750610000'>,\n",
       " <DirEntry 'outputs-1585750620000'>,\n",
       " <DirEntry 'outputs-1585750630000'>,\n",
       " <DirEntry 'outputs-1585750640000'>,\n",
       " <DirEntry 'outputs-1585750650000'>,\n",
       " <DirEntry 'outputs-1585750660000'>,\n",
       " <DirEntry 'outputs-1585750670000'>,\n",
       " <DirEntry 'outputs-1585750680000'>,\n",
       " <DirEntry 'outputs-1585750690000'>,\n",
       " <DirEntry 'outputs-1585750700000'>,\n",
       " <DirEntry 'outputs-1585750710000'>,\n",
       " <DirEntry 'outputs-1585750720000'>,\n",
       " <DirEntry 'outputs-1585750730000'>,\n",
       " <DirEntry 'outputs-1585750740000'>,\n",
       " <DirEntry 'outputs-1585750750000'>,\n",
       " <DirEntry 'outputs-1585750760000'>,\n",
       " <DirEntry 'outputs-1585750770000'>,\n",
       " <DirEntry 'outputs-1585750780000'>,\n",
       " <DirEntry 'outputs-1585750790000'>,\n",
       " <DirEntry 'outputs-1585750800000'>,\n",
       " <DirEntry 'outputs-1585750810000'>,\n",
       " <DirEntry 'outputs-1585750820000'>,\n",
       " <DirEntry 'outputs-1585750830000'>,\n",
       " <DirEntry 'outputs-1585750840000'>,\n",
       " <DirEntry 'outputs-1585750850000'>,\n",
       " <DirEntry 'outputs-1585750860000'>,\n",
       " <DirEntry 'outputs-1585750870000'>,\n",
       " <DirEntry 'outputs-1585750880000'>,\n",
       " <DirEntry 'outputs-1585750890000'>,\n",
       " <DirEntry 'outputs-1585750900000'>,\n",
       " <DirEntry 'outputs-1585750910000'>,\n",
       " <DirEntry 'outputs-1585750920000'>,\n",
       " <DirEntry 'outputs-1585750930000'>,\n",
       " <DirEntry 'outputs-1585750940000'>,\n",
       " <DirEntry 'outputs-1585750950000'>,\n",
       " <DirEntry 'outputs-1585750960000'>,\n",
       " <DirEntry 'outputs-1585750970000'>,\n",
       " <DirEntry 'outputs-1585750980000'>,\n",
       " <DirEntry 'outputs-1585750990000'>,\n",
       " <DirEntry 'outputs-1585751000000'>,\n",
       " <DirEntry 'outputs-1585751010000'>,\n",
       " <DirEntry 'outputs-1585751020000'>,\n",
       " <DirEntry 'outputs-1585751030000'>,\n",
       " <DirEntry 'outputs-1585751040000'>,\n",
       " <DirEntry 'outputs-1585751050000'>,\n",
       " <DirEntry 'outputs-1585751060000'>,\n",
       " <DirEntry 'outputs-1585751070000'>,\n",
       " <DirEntry 'outputs-1585751080000'>,\n",
       " <DirEntry 'outputs-1585751090000'>,\n",
       " <DirEntry 'outputs-1585751100000'>,\n",
       " <DirEntry 'outputs-1585751110000'>,\n",
       " <DirEntry 'outputs-1585751120000'>,\n",
       " <DirEntry 'outputs-1585751130000'>,\n",
       " <DirEntry 'outputs-1585751140000'>,\n",
       " <DirEntry 'outputs-1585751150000'>,\n",
       " <DirEntry 'outputs-1585751160000'>,\n",
       " <DirEntry 'outputs-1585751170000'>,\n",
       " <DirEntry 'outputs-1585751180000'>,\n",
       " <DirEntry 'outputs-1585751190000'>,\n",
       " <DirEntry 'outputs-1585751200000'>,\n",
       " <DirEntry 'outputs-1585751210000'>,\n",
       " <DirEntry 'outputs-1585751220000'>,\n",
       " <DirEntry 'outputs-1585751230000'>,\n",
       " <DirEntry 'outputs-1585751240000'>,\n",
       " <DirEntry 'outputs-1585751250000'>,\n",
       " <DirEntry 'outputs-1585751260000'>,\n",
       " <DirEntry 'outputs-1585751270000'>,\n",
       " <DirEntry 'outputs-1585751280000'>,\n",
       " <DirEntry 'outputs-1585751290000'>,\n",
       " <DirEntry 'outputs-1585751300000'>,\n",
       " <DirEntry 'outputs-1585751310000'>,\n",
       " <DirEntry 'outputs-1585751320000'>,\n",
       " <DirEntry 'outputs-1585751330000'>,\n",
       " <DirEntry 'outputs-1585751340000'>,\n",
       " <DirEntry 'outputs-1585751350000'>,\n",
       " <DirEntry 'outputs-1585751360000'>,\n",
       " <DirEntry 'outputs-1585751370000'>,\n",
       " <DirEntry 'outputs-1585751380000'>,\n",
       " <DirEntry 'outputs-1585751390000'>,\n",
       " <DirEntry 'outputs-1585751400000'>,\n",
       " <DirEntry 'outputs-1585751410000'>,\n",
       " <DirEntry 'outputs-1585751420000'>,\n",
       " <DirEntry 'outputs-1585751430000'>,\n",
       " <DirEntry 'outputs-1585751440000'>,\n",
       " <DirEntry 'outputs-1585751450000'>,\n",
       " <DirEntry 'outputs-1585751460000'>,\n",
       " <DirEntry 'outputs-1585751470000'>,\n",
       " <DirEntry 'outputs-1585751480000'>,\n",
       " <DirEntry 'outputs-1585751490000'>,\n",
       " <DirEntry 'outputs-1585751500000'>,\n",
       " <DirEntry 'outputs-1585751510000'>,\n",
       " <DirEntry 'outputs-1585751520000'>,\n",
       " <DirEntry 'outputs-1585751530000'>,\n",
       " <DirEntry 'outputs-1585751540000'>,\n",
       " <DirEntry 'outputs-1585751550000'>,\n",
       " <DirEntry 'outputs-1585751560000'>,\n",
       " <DirEntry 'outputs-1585751570000'>,\n",
       " <DirEntry 'outputs-1585751580000'>,\n",
       " <DirEntry 'outputs-1585751590000'>,\n",
       " <DirEntry 'outputs-1585751600000'>,\n",
       " <DirEntry 'outputs-1585751610000'>,\n",
       " <DirEntry 'outputs-1585751620000'>,\n",
       " <DirEntry 'outputs-1585751630000'>,\n",
       " <DirEntry 'outputs-1585751640000'>,\n",
       " <DirEntry 'outputs-1585751650000'>,\n",
       " <DirEntry 'outputs-1585751660000'>,\n",
       " <DirEntry 'outputs-1585751670000'>,\n",
       " <DirEntry 'outputs-1585751680000'>,\n",
       " <DirEntry 'outputs-1585751690000'>,\n",
       " <DirEntry 'outputs-1585751700000'>,\n",
       " <DirEntry 'outputs-1585751710000'>,\n",
       " <DirEntry 'outputs-1585751720000'>,\n",
       " <DirEntry 'outputs-1585751730000'>,\n",
       " <DirEntry 'outputs-1585751740000'>,\n",
       " <DirEntry 'outputs-1585751750000'>,\n",
       " <DirEntry 'outputs-1585751760000'>,\n",
       " <DirEntry 'outputs-1585751770000'>,\n",
       " <DirEntry 'outputs-1585751780000'>,\n",
       " <DirEntry 'outputs-1585751790000'>,\n",
       " <DirEntry 'outputs-1585751800000'>,\n",
       " <DirEntry 'outputs-1585751810000'>,\n",
       " <DirEntry 'outputs-1585751820000'>,\n",
       " <DirEntry 'outputs-1585751830000'>,\n",
       " <DirEntry 'outputs-1585751840000'>,\n",
       " <DirEntry 'outputs-1585751850000'>,\n",
       " <DirEntry 'outputs-1585751860000'>,\n",
       " <DirEntry 'outputs-1585751870000'>,\n",
       " <DirEntry 'outputs-1585751880000'>,\n",
       " <DirEntry 'outputs-1585751890000'>,\n",
       " <DirEntry 'outputs-1585751900000'>,\n",
       " <DirEntry 'outputs-1585751910000'>,\n",
       " <DirEntry 'outputs-1585751920000'>,\n",
       " <DirEntry 'outputs-1585751930000'>,\n",
       " <DirEntry 'outputs-1585751940000'>,\n",
       " <DirEntry 'outputs-1585751950000'>,\n",
       " <DirEntry 'outputs-1585751960000'>,\n",
       " <DirEntry 'outputs-1585751970000'>,\n",
       " <DirEntry 'outputs-1585751980000'>,\n",
       " <DirEntry 'outputs-1585751990000'>,\n",
       " <DirEntry 'outputs-1585752000000'>,\n",
       " <DirEntry 'outputs-1585752010000'>,\n",
       " <DirEntry 'outputs-1585752020000'>,\n",
       " <DirEntry 'outputs-1585752030000'>,\n",
       " <DirEntry 'outputs-1585752040000'>,\n",
       " <DirEntry 'outputs-1585752050000'>,\n",
       " <DirEntry 'outputs-1585752060000'>,\n",
       " <DirEntry 'outputs-1585752070000'>,\n",
       " <DirEntry 'outputs-1585752080000'>,\n",
       " <DirEntry 'outputs-1585752090000'>,\n",
       " <DirEntry 'outputs-1585752100000'>,\n",
       " <DirEntry 'outputs-1585752110000'>,\n",
       " <DirEntry 'outputs-1585752120000'>,\n",
       " <DirEntry 'outputs-1585752130000'>,\n",
       " <DirEntry 'outputs-1585752140000'>,\n",
       " <DirEntry 'outputs-1585752150000'>,\n",
       " <DirEntry 'outputs-1585752160000'>,\n",
       " <DirEntry 'outputs-1585752170000'>,\n",
       " <DirEntry 'outputs-1585752180000'>,\n",
       " <DirEntry 'outputs-1585752190000'>,\n",
       " <DirEntry 'outputs-1585752200000'>,\n",
       " <DirEntry 'outputs-1585752210000'>,\n",
       " <DirEntry 'outputs-1585752220000'>,\n",
       " <DirEntry 'outputs-1585752230000'>,\n",
       " <DirEntry 'outputs-1585752240000'>,\n",
       " <DirEntry 'outputs-1585752250000'>,\n",
       " <DirEntry 'outputs-1585752260000'>,\n",
       " <DirEntry 'outputs-1585752270000'>,\n",
       " <DirEntry 'outputs-1585752280000'>,\n",
       " <DirEntry 'outputs-1585752290000'>,\n",
       " <DirEntry 'outputs-1585752300000'>,\n",
       " <DirEntry 'outputs-1585752310000'>,\n",
       " <DirEntry 'outputs-1585752320000'>,\n",
       " <DirEntry 'outputs-1585752330000'>,\n",
       " <DirEntry 'outputs-1585752340000'>,\n",
       " <DirEntry 'outputs-1585752350000'>,\n",
       " <DirEntry 'outputs-1585752360000'>,\n",
       " <DirEntry 'outputs-1585752370000'>,\n",
       " <DirEntry 'outputs-1585752380000'>,\n",
       " <DirEntry 'outputs-1585752390000'>,\n",
       " <DirEntry 'outputs-1585752400000'>,\n",
       " <DirEntry 'outputs-1585752410000'>,\n",
       " <DirEntry 'outputs-1585752420000'>,\n",
       " <DirEntry 'outputs-1585752430000'>,\n",
       " <DirEntry 'outputs-1585752440000'>,\n",
       " <DirEntry 'outputs-1585752450000'>,\n",
       " <DirEntry 'outputs-1585752460000'>,\n",
       " <DirEntry 'outputs-1585752470000'>,\n",
       " <DirEntry 'outputs-1585752480000'>,\n",
       " <DirEntry 'outputs-1585752490000'>,\n",
       " <DirEntry 'outputs-1585752500000'>,\n",
       " <DirEntry 'outputs-1585752510000'>,\n",
       " <DirEntry 'outputs-1585752520000'>,\n",
       " <DirEntry 'outputs-1585752530000'>,\n",
       " <DirEntry 'outputs-1585752540000'>,\n",
       " <DirEntry 'outputs-1585752550000'>,\n",
       " <DirEntry 'outputs-1585752560000'>,\n",
       " <DirEntry 'outputs-1585752570000'>,\n",
       " <DirEntry 'outputs-1585752580000'>,\n",
       " <DirEntry 'outputs-1585752590000'>,\n",
       " <DirEntry 'outputs-1585752600000'>,\n",
       " <DirEntry 'outputs-1585752610000'>,\n",
       " <DirEntry 'outputs-1585752620000'>,\n",
       " <DirEntry 'outputs-1585752630000'>,\n",
       " <DirEntry 'outputs-1585752640000'>,\n",
       " <DirEntry 'outputs-1585752650000'>,\n",
       " <DirEntry 'outputs-1585752660000'>,\n",
       " <DirEntry 'outputs-1585752670000'>,\n",
       " <DirEntry 'outputs-1585752680000'>,\n",
       " <DirEntry 'outputs-1585752690000'>,\n",
       " <DirEntry 'outputs-1585752700000'>,\n",
       " <DirEntry 'outputs-1585752710000'>,\n",
       " <DirEntry 'outputs-1585752720000'>,\n",
       " <DirEntry 'outputs-1585752730000'>,\n",
       " <DirEntry 'outputs-1585752740000'>,\n",
       " <DirEntry 'outputs-1585752750000'>,\n",
       " <DirEntry 'outputs-1585752760000'>,\n",
       " <DirEntry 'outputs-1585752770000'>,\n",
       " <DirEntry 'outputs-1585752780000'>,\n",
       " <DirEntry 'outputs-1585752790000'>,\n",
       " <DirEntry 'outputs-1585752800000'>,\n",
       " <DirEntry 'outputs-1585752810000'>,\n",
       " <DirEntry 'outputs-1585752820000'>,\n",
       " <DirEntry 'outputs-1585752830000'>,\n",
       " <DirEntry 'outputs-1585752840000'>,\n",
       " <DirEntry 'outputs-1585752850000'>,\n",
       " <DirEntry 'outputs-1585752860000'>,\n",
       " <DirEntry 'outputs-1585752870000'>,\n",
       " <DirEntry 'outputs-1585752880000'>,\n",
       " <DirEntry 'outputs-1585752890000'>,\n",
       " <DirEntry 'outputs-1585752900000'>,\n",
       " <DirEntry 'outputs-1585752910000'>,\n",
       " <DirEntry 'outputs-1585752920000'>,\n",
       " <DirEntry 'outputs-1585752930000'>,\n",
       " <DirEntry 'outputs-1585752940000'>,\n",
       " <DirEntry 'outputs-1585752950000'>,\n",
       " <DirEntry 'outputs-1585752960000'>,\n",
       " <DirEntry 'outputs-1585752970000'>,\n",
       " <DirEntry 'outputs-1585752980000'>,\n",
       " <DirEntry 'outputs-1585752990000'>,\n",
       " <DirEntry 'outputs-1585753000000'>,\n",
       " <DirEntry 'outputs-1585753010000'>,\n",
       " <DirEntry 'outputs-1585753020000'>,\n",
       " <DirEntry 'outputs-1585753030000'>,\n",
       " <DirEntry 'outputs-1585753040000'>,\n",
       " <DirEntry 'outputs-1585753050000'>,\n",
       " <DirEntry 'outputs-1585753060000'>,\n",
       " <DirEntry 'outputs-1585753070000'>,\n",
       " <DirEntry 'outputs-1585753080000'>,\n",
       " <DirEntry 'outputs-1585753090000'>,\n",
       " <DirEntry 'outputs-1585753100000'>,\n",
       " <DirEntry 'outputs-1585753110000'>,\n",
       " <DirEntry 'outputs-1585753120000'>,\n",
       " <DirEntry 'outputs-1585753130000'>,\n",
       " <DirEntry 'outputs-1585753140000'>,\n",
       " <DirEntry 'outputs-1585753150000'>,\n",
       " <DirEntry 'outputs-1585753160000'>,\n",
       " <DirEntry 'outputs-1585753170000'>,\n",
       " <DirEntry 'outputs-1585753180000'>,\n",
       " <DirEntry 'outputs-1585753190000'>,\n",
       " <DirEntry 'outputs-1585753200000'>,\n",
       " <DirEntry 'outputs-1585753210000'>,\n",
       " <DirEntry 'outputs-1585753220000'>,\n",
       " <DirEntry 'outputs-1585753230000'>,\n",
       " <DirEntry 'outputs-1585753240000'>,\n",
       " <DirEntry 'outputs-1585753250000'>,\n",
       " <DirEntry 'outputs-1585753260000'>,\n",
       " <DirEntry 'outputs-1585753270000'>,\n",
       " <DirEntry 'outputs-1585753280000'>,\n",
       " <DirEntry 'outputs-1585753290000'>,\n",
       " <DirEntry 'outputs-1585753300000'>,\n",
       " <DirEntry 'outputs-1585753310000'>,\n",
       " <DirEntry 'outputs-1585753320000'>,\n",
       " <DirEntry 'outputs-1585753330000'>,\n",
       " <DirEntry 'outputs-1585753340000'>,\n",
       " <DirEntry 'outputs-1585753350000'>,\n",
       " <DirEntry 'outputs-1585753360000'>,\n",
       " <DirEntry 'outputs-1585753370000'>,\n",
       " <DirEntry 'outputs-1585753380000'>,\n",
       " <DirEntry 'outputs-1585753390000'>,\n",
       " <DirEntry 'outputs-1585753400000'>,\n",
       " <DirEntry 'outputs-1585753410000'>,\n",
       " <DirEntry 'outputs-1585753420000'>,\n",
       " <DirEntry 'outputs-1585753430000'>,\n",
       " <DirEntry 'outputs-1585753440000'>,\n",
       " <DirEntry 'outputs-1585753450000'>,\n",
       " <DirEntry 'outputs-1585753460000'>,\n",
       " <DirEntry 'outputs-1585753470000'>,\n",
       " <DirEntry 'outputs-1585753480000'>,\n",
       " <DirEntry 'outputs-1585753490000'>,\n",
       " <DirEntry 'outputs-1585753500000'>,\n",
       " <DirEntry 'outputs-1585753510000'>,\n",
       " <DirEntry 'outputs-1585753520000'>,\n",
       " <DirEntry 'outputs-1585753530000'>,\n",
       " <DirEntry 'outputs-1585753540000'>,\n",
       " <DirEntry 'outputs-1585753550000'>,\n",
       " <DirEntry 'outputs-1585753560000'>,\n",
       " <DirEntry 'outputs-1585753570000'>,\n",
       " <DirEntry 'outputs-1585753580000'>,\n",
       " <DirEntry 'outputs-1585753590000'>,\n",
       " <DirEntry 'outputs-1585753600000'>,\n",
       " <DirEntry 'outputs-1585753610000'>,\n",
       " <DirEntry 'outputs-1585753620000'>,\n",
       " <DirEntry 'outputs-1585753630000'>,\n",
       " <DirEntry 'outputs-1585753640000'>,\n",
       " <DirEntry 'outputs-1585753650000'>,\n",
       " <DirEntry 'outputs-1585753660000'>,\n",
       " <DirEntry 'outputs-1585753670000'>,\n",
       " <DirEntry 'outputs-1585753680000'>,\n",
       " <DirEntry 'outputs-1585753690000'>,\n",
       " <DirEntry 'outputs-1585753700000'>,\n",
       " <DirEntry 'outputs-1585753710000'>,\n",
       " <DirEntry 'outputs-1585753720000'>,\n",
       " <DirEntry 'outputs-1585753730000'>,\n",
       " <DirEntry 'outputs-1585753740000'>,\n",
       " <DirEntry 'outputs-1585753750000'>,\n",
       " <DirEntry 'outputs-1585753760000'>,\n",
       " <DirEntry 'outputs-1585753770000'>,\n",
       " <DirEntry 'outputs-1585753780000'>,\n",
       " <DirEntry 'outputs-1585753790000'>,\n",
       " <DirEntry 'outputs-1585753800000'>,\n",
       " <DirEntry 'outputs-1585753810000'>,\n",
       " <DirEntry 'outputs-1585753820000'>,\n",
       " <DirEntry 'outputs-1585753830000'>,\n",
       " <DirEntry 'outputs-1585753840000'>,\n",
       " <DirEntry 'outputs-1585753850000'>,\n",
       " <DirEntry 'outputs-1585753860000'>,\n",
       " <DirEntry 'outputs-1585753870000'>,\n",
       " <DirEntry 'outputs-1585753880000'>,\n",
       " <DirEntry 'outputs-1585753890000'>,\n",
       " <DirEntry 'outputs-1585753900000'>,\n",
       " <DirEntry 'outputs-1585753910000'>,\n",
       " <DirEntry 'outputs-1585753920000'>,\n",
       " <DirEntry 'outputs-1585753930000'>,\n",
       " <DirEntry 'outputs-1585753940000'>,\n",
       " <DirEntry 'outputs-1585753950000'>,\n",
       " <DirEntry 'outputs-1585753960000'>,\n",
       " <DirEntry 'outputs-1585753970000'>,\n",
       " <DirEntry 'outputs-1585753980000'>,\n",
       " <DirEntry 'outputs-1585753990000'>,\n",
       " <DirEntry 'outputs-1585754000000'>,\n",
       " <DirEntry 'outputs-1585754010000'>,\n",
       " <DirEntry 'outputs-1585754020000'>,\n",
       " <DirEntry 'outputs-1585754030000'>,\n",
       " <DirEntry 'outputs-1585754040000'>,\n",
       " <DirEntry 'outputs-1585754050000'>,\n",
       " <DirEntry 'outputs-1585754060000'>,\n",
       " <DirEntry 'outputs-1585754070000'>,\n",
       " <DirEntry 'outputs-1585754080000'>,\n",
       " <DirEntry 'outputs-1585754090000'>,\n",
       " <DirEntry 'outputs-1585754100000'>,\n",
       " <DirEntry 'outputs-1585754110000'>,\n",
       " <DirEntry 'outputs-1585754120000'>,\n",
       " <DirEntry 'outputs-1585754130000'>,\n",
       " <DirEntry 'outputs-1585754140000'>,\n",
       " <DirEntry 'outputs-1585754150000'>,\n",
       " <DirEntry 'outputs-1585754160000'>,\n",
       " <DirEntry 'outputs-1585754170000'>,\n",
       " <DirEntry 'outputs-1585754180000'>,\n",
       " <DirEntry 'outputs-1585754190000'>,\n",
       " <DirEntry 'outputs-1585754200000'>,\n",
       " <DirEntry 'outputs-1585754210000'>,\n",
       " <DirEntry 'outputs-1585754220000'>,\n",
       " <DirEntry 'outputs-1585754230000'>,\n",
       " <DirEntry 'outputs-1585754240000'>,\n",
       " <DirEntry 'outputs-1585754250000'>,\n",
       " <DirEntry 'outputs-1585754260000'>,\n",
       " <DirEntry 'outputs-1585754270000'>,\n",
       " <DirEntry 'outputs-1585754280000'>,\n",
       " <DirEntry 'outputs-1585754290000'>,\n",
       " <DirEntry 'outputs-1585754300000'>,\n",
       " <DirEntry 'outputs-1585754310000'>,\n",
       " <DirEntry 'outputs-1585754320000'>,\n",
       " <DirEntry 'outputs-1585754330000'>,\n",
       " <DirEntry 'outputs-1585754340000'>,\n",
       " <DirEntry 'outputs-1585754350000'>,\n",
       " <DirEntry 'outputs-1585754360000'>,\n",
       " <DirEntry 'outputs-1585754370000'>,\n",
       " <DirEntry 'outputs-1585754380000'>,\n",
       " <DirEntry 'outputs-1585754390000'>,\n",
       " <DirEntry 'outputs-1585754400000'>,\n",
       " <DirEntry 'outputs-1585754410000'>,\n",
       " <DirEntry 'outputs-1585754420000'>,\n",
       " <DirEntry 'outputs-1585754430000'>,\n",
       " <DirEntry 'outputs-1585754440000'>,\n",
       " <DirEntry 'outputs-1585754450000'>,\n",
       " <DirEntry 'outputs-1585754460000'>,\n",
       " <DirEntry 'outputs-1585754470000'>,\n",
       " <DirEntry 'outputs-1585754480000'>,\n",
       " <DirEntry 'outputs-1585754490000'>,\n",
       " <DirEntry 'outputs-1585754500000'>,\n",
       " <DirEntry 'outputs-1585754510000'>,\n",
       " <DirEntry 'outputs-1585754520000'>,\n",
       " <DirEntry 'outputs-1585754530000'>,\n",
       " <DirEntry 'outputs-1585754540000'>,\n",
       " <DirEntry 'outputs-1585754550000'>,\n",
       " <DirEntry 'outputs-1585754560000'>,\n",
       " <DirEntry 'outputs-1585754570000'>,\n",
       " <DirEntry 'outputs-1585754580000'>,\n",
       " <DirEntry 'outputs-1585754590000'>,\n",
       " <DirEntry 'outputs-1585754600000'>,\n",
       " <DirEntry 'outputs-1585754610000'>,\n",
       " <DirEntry 'outputs-1585754620000'>,\n",
       " <DirEntry 'outputs-1585754630000'>,\n",
       " <DirEntry 'outputs-1585754640000'>,\n",
       " <DirEntry 'outputs-1585754650000'>,\n",
       " <DirEntry 'outputs-1585754660000'>,\n",
       " <DirEntry 'outputs-1585754670000'>,\n",
       " <DirEntry 'outputs-1585754680000'>,\n",
       " <DirEntry 'outputs-1585754690000'>,\n",
       " <DirEntry 'outputs-1585754700000'>,\n",
       " <DirEntry 'outputs-1585754710000'>,\n",
       " <DirEntry 'outputs-1585754720000'>,\n",
       " <DirEntry 'outputs-1585754730000'>,\n",
       " <DirEntry 'outputs-1585754740000'>,\n",
       " <DirEntry 'outputs-1585754750000'>,\n",
       " <DirEntry 'outputs-1585754760000'>,\n",
       " <DirEntry 'outputs-1585754770000'>,\n",
       " <DirEntry 'outputs-1585754780000'>,\n",
       " <DirEntry 'outputs-1585754790000'>,\n",
       " <DirEntry 'outputs-1585754800000'>,\n",
       " <DirEntry 'outputs-1585754810000'>,\n",
       " <DirEntry 'outputs-1585754820000'>,\n",
       " <DirEntry 'outputs-1585754830000'>,\n",
       " <DirEntry 'outputs-1585754840000'>,\n",
       " <DirEntry 'outputs-1585754850000'>,\n",
       " <DirEntry 'outputs-1585754860000'>,\n",
       " <DirEntry 'outputs-1585754870000'>,\n",
       " <DirEntry 'outputs-1585754880000'>,\n",
       " <DirEntry 'outputs-1585754890000'>,\n",
       " <DirEntry 'outputs-1585754900000'>,\n",
       " <DirEntry 'outputs-1585754910000'>,\n",
       " <DirEntry 'outputs-1585754920000'>,\n",
       " <DirEntry 'outputs-1585754930000'>,\n",
       " <DirEntry 'outputs-1585754940000'>,\n",
       " <DirEntry 'outputs-1585754950000'>,\n",
       " <DirEntry 'outputs-1585754960000'>,\n",
       " <DirEntry 'outputs-1585754970000'>,\n",
       " <DirEntry 'outputs-1585754980000'>,\n",
       " <DirEntry 'outputs-1585754990000'>,\n",
       " <DirEntry 'outputs-1585755000000'>,\n",
       " <DirEntry 'outputs-1585755010000'>,\n",
       " <DirEntry 'outputs-1585755020000'>,\n",
       " <DirEntry 'outputs-1585755030000'>,\n",
       " <DirEntry 'outputs-1585755040000'>,\n",
       " <DirEntry 'outputs-1585755050000'>,\n",
       " <DirEntry 'outputs-1585755060000'>,\n",
       " <DirEntry 'outputs-1585755070000'>,\n",
       " <DirEntry 'outputs-1585755080000'>,\n",
       " <DirEntry 'outputs-1585755090000'>,\n",
       " <DirEntry 'outputs-1585755100000'>,\n",
       " <DirEntry 'outputs-1585755110000'>,\n",
       " <DirEntry 'outputs-1585755120000'>,\n",
       " <DirEntry 'outputs-1585755130000'>,\n",
       " <DirEntry 'outputs-1585755140000'>,\n",
       " <DirEntry 'outputs-1585755150000'>,\n",
       " <DirEntry 'outputs-1585755160000'>,\n",
       " <DirEntry 'outputs-1585755170000'>,\n",
       " <DirEntry 'outputs-1585755180000'>,\n",
       " <DirEntry 'outputs-1585755190000'>,\n",
       " <DirEntry 'outputs-1585755200000'>,\n",
       " <DirEntry 'outputs-1585755210000'>,\n",
       " <DirEntry 'outputs-1585755220000'>,\n",
       " <DirEntry 'outputs-1585755230000'>,\n",
       " <DirEntry 'outputs-1585755240000'>,\n",
       " <DirEntry 'outputs-1585755250000'>,\n",
       " <DirEntry 'outputs-1585755260000'>,\n",
       " <DirEntry 'outputs-1585755270000'>,\n",
       " <DirEntry 'outputs-1585755280000'>,\n",
       " <DirEntry 'outputs-1585755290000'>,\n",
       " <DirEntry 'outputs-1585755300000'>,\n",
       " <DirEntry 'outputs-1585755310000'>,\n",
       " <DirEntry 'outputs-1585755320000'>,\n",
       " <DirEntry 'outputs-1585755330000'>,\n",
       " <DirEntry 'outputs-1585755340000'>,\n",
       " <DirEntry 'outputs-1585755350000'>,\n",
       " <DirEntry 'outputs-1585755360000'>,\n",
       " <DirEntry 'outputs-1585755370000'>,\n",
       " <DirEntry 'outputs-1585755380000'>,\n",
       " <DirEntry 'outputs-1585755390000'>,\n",
       " <DirEntry 'outputs-1585755400000'>,\n",
       " <DirEntry 'outputs-1585755410000'>,\n",
       " <DirEntry 'outputs-1585755420000'>,\n",
       " <DirEntry 'outputs-1585755430000'>,\n",
       " <DirEntry 'outputs-1585755440000'>,\n",
       " <DirEntry 'outputs-1585755450000'>,\n",
       " <DirEntry 'outputs-1585755460000'>,\n",
       " <DirEntry 'outputs-1585755470000'>,\n",
       " <DirEntry 'outputs-1585755480000'>,\n",
       " <DirEntry 'outputs-1585755490000'>,\n",
       " <DirEntry 'outputs-1585755500000'>,\n",
       " <DirEntry 'outputs-1585755510000'>,\n",
       " <DirEntry 'outputs-1585755520000'>,\n",
       " <DirEntry 'outputs-1585755530000'>,\n",
       " <DirEntry 'outputs-1585755540000'>,\n",
       " <DirEntry 'outputs-1585755550000'>,\n",
       " <DirEntry 'outputs-1585755560000'>,\n",
       " <DirEntry 'outputs-1585755570000'>,\n",
       " <DirEntry 'outputs-1585755580000'>,\n",
       " <DirEntry 'outputs-1585755590000'>,\n",
       " <DirEntry 'outputs-1585755600000'>,\n",
       " <DirEntry 'outputs-1585755610000'>,\n",
       " <DirEntry 'outputs-1585755620000'>,\n",
       " <DirEntry 'outputs-1585755630000'>,\n",
       " <DirEntry 'outputs-1585755640000'>,\n",
       " <DirEntry 'outputs-1585755650000'>,\n",
       " <DirEntry 'outputs-1585755660000'>,\n",
       " <DirEntry 'outputs-1585755670000'>,\n",
       " <DirEntry 'outputs-1585755680000'>,\n",
       " <DirEntry 'outputs-1585755690000'>,\n",
       " <DirEntry 'outputs-1585755700000'>,\n",
       " <DirEntry 'outputs-1585755710000'>,\n",
       " <DirEntry 'outputs-1585755720000'>,\n",
       " <DirEntry 'outputs-1585755730000'>,\n",
       " <DirEntry 'outputs-1585755740000'>,\n",
       " <DirEntry 'outputs-1585755750000'>,\n",
       " <DirEntry 'outputs-1585755760000'>,\n",
       " <DirEntry 'outputs-1585755770000'>,\n",
       " <DirEntry 'outputs-1585755780000'>,\n",
       " <DirEntry 'outputs-1585755790000'>,\n",
       " <DirEntry 'outputs-1585755800000'>,\n",
       " <DirEntry 'outputs-1585755810000'>,\n",
       " <DirEntry 'outputs-1585755820000'>,\n",
       " <DirEntry 'outputs-1585755830000'>,\n",
       " <DirEntry 'outputs-1585755840000'>,\n",
       " <DirEntry 'outputs-1585755850000'>,\n",
       " <DirEntry 'outputs-1585755860000'>,\n",
       " <DirEntry 'outputs-1585755870000'>,\n",
       " <DirEntry 'outputs-1585755880000'>,\n",
       " <DirEntry 'outputs-1585755890000'>,\n",
       " <DirEntry 'outputs-1585755900000'>,\n",
       " <DirEntry 'outputs-1585755910000'>,\n",
       " <DirEntry 'outputs-1585755920000'>,\n",
       " <DirEntry 'outputs-1585755930000'>,\n",
       " <DirEntry 'outputs-1585755940000'>,\n",
       " <DirEntry 'outputs-1585755950000'>,\n",
       " <DirEntry 'outputs-1585755960000'>,\n",
       " <DirEntry 'outputs-1585755970000'>,\n",
       " <DirEntry 'outputs-1585755980000'>,\n",
       " <DirEntry 'outputs-1585755990000'>,\n",
       " <DirEntry 'outputs-1585756000000'>,\n",
       " <DirEntry 'outputs-1585756010000'>,\n",
       " <DirEntry 'outputs-1585756020000'>,\n",
       " <DirEntry 'outputs-1585756030000'>,\n",
       " <DirEntry 'outputs-1585756040000'>,\n",
       " <DirEntry 'outputs-1585756050000'>,\n",
       " <DirEntry 'outputs-1585756060000'>,\n",
       " <DirEntry 'outputs-1585756070000'>,\n",
       " <DirEntry 'outputs-1585756080000'>,\n",
       " <DirEntry 'outputs-1585756090000'>,\n",
       " <DirEntry 'outputs-1585756100000'>,\n",
       " <DirEntry 'outputs-1585756110000'>,\n",
       " <DirEntry 'outputs-1585756120000'>,\n",
       " <DirEntry 'outputs-1585756130000'>,\n",
       " <DirEntry 'outputs-1585756140000'>,\n",
       " <DirEntry 'outputs-1585756150000'>,\n",
       " <DirEntry 'outputs-1585756160000'>,\n",
       " <DirEntry 'outputs-1585756170000'>,\n",
       " <DirEntry 'outputs-1585756180000'>,\n",
       " <DirEntry 'outputs-1585756190000'>,\n",
       " <DirEntry 'outputs-1585756200000'>,\n",
       " <DirEntry 'outputs-1585756210000'>,\n",
       " <DirEntry 'outputs-1585756220000'>,\n",
       " <DirEntry 'outputs-1585756230000'>,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = list(os.scandir(cwd))\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- comment: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- text_new: string (nullable = true)\n",
      " |-- text_old: string (nullable = true)\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(cwd + '//' + dirs[4000])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(len(dirs)):\\n    df_tojoin = spark.read.format(\"json\").load(cwd + \\'//\\' + dirs[i])\\n    dataframe = df.union(df_tojoin)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(len(dirs)):\n",
    "    df_tojoin = spark.read.format(\"json\").load(cwd + '//' + dirs[i])\n",
    "    dataframe = df.union(df_tojoin)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\streaming_1a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(cwd + '//*//*' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- comment: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- text_new: string (nullable = true)\n",
      " |-- text_old: string (nullable = true)\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "udfMake_Diff = udf(make_diff, StringType())\n",
    "df_new = df.withColumn(\"diff\", udfMake_Diff(\"text_old\", \"text_new\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"diff\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenizer2 = RegexTokenizer(inputCol=\"diff\", outputCol=\"words\", pattern=\"\\\\d\")\n",
    "df_tknz = tokenizer.transform(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "df_filter = remover.transform(df_tknz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- comment: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- text_new: string (nullable = true)\n",
      " |-- text_old: string (nullable = true)\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      " |-- diff: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vectors: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "model = cv.fit(df_filter)\n",
    "df_cv = model.transform(df_filter)\n",
    "df_cv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"vectors\", outputCol=\"features\")\n",
    "idfModel = idf.fit(df_cv)\n",
    "df_tfidf = idfModel.transform(df_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- comment: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- text_new: string (nullable = true)\n",
      " |-- text_old: string (nullable = true)\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      " |-- diff: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vectors: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tfidf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Algorithm\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"target\")\n",
    "data = label_indexer.fit(df_tfidf).transform(df_tfidf).select(\"target\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = data.randomSplit([0.70, 0.30])\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt = DecisionTreeClassifier(labelCol=\"target\", featuresCol=\"features\")\n",
    "#modeldt = dt.fit(train)\n",
    "#predictiondt = modeldt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictiondt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6176191122573639\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"target\", featuresCol=\"features\")\n",
    "model_nb = nb.fit(train)\n",
    "predictions = model_nb.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/C:/Users/u0115374/Documents/PhD/Courses/Big Data/SparkStream/train already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o329.parquet.\n: org.apache.spark.sql.AnalysisException: path file:/C:/Users/u0115374/Documents/PhD/Courses/Big Data/SparkStream/train already exists.;\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0f9a20795383>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#data.write.parquet('data_1a.parquet')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#test.saveAsTextFile('test.txt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'path file:/C:/Users/u0115374/Documents/PhD/Courses/Big Data/SparkStream/train already exists.;'"
     ]
    }
   ],
   "source": [
    "#data.write.parquet('data_1a.parquet')\n",
    "train.write.parquet(\"train\")\n",
    "#test.saveAsTextFile('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\streaming_1a'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1424.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 32511, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Desktop\\training_data.parquet\\_temporary\\0\\_temporary\\attempt_20200509182141_0032_m_000000_32511\\part-00000-8da80388-9332-4557-b5da-5e9572bb013c-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Desktop\\training_data.parquet\\_temporary\\0\\_temporary\\attempt_20200509182141_0032_m_000000_32511\\part-00000-8da80388-9332-4557-b5da-5e9572bb013c-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e5a3c3ee0d40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\u0115374\\\\Desktop\\\\training_data.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1424.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 32511, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Desktop\\training_data.parquet\\_temporary\\0\\_temporary\\attempt_20200509182141_0032_m_000000_32511\\part-00000-8da80388-9332-4557-b5da-5e9572bb013c-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\u0115374\\Desktop\\training_data.parquet\\_temporary\\0\\_temporary\\attempt_20200509182141_0032_m_000000_32511\\part-00000-8da80388-9332-4557-b5da-5e9572bb013c-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "train.write.parquet(\"C:\\\\Users\\\\u0115374\\\\Desktop\\\\training_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1969.save.\n: java.io.IOException: Path C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\Assignment3\\streaming_1a already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\r\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:702)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:179)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-7fe69e29b536>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1969.save.\n: java.io.IOException: Path C:\\Users\\u0115374\\Documents\\PhD\\Courses\\Big Data\\Assignment3\\streaming_1a already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\r\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:702)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:179)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "model_nb.save(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
