{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u0115374\\Desktop\\spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from difflib import unified_diff\n",
    "from pyspark import ml\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml import Pipeline, Transformer, PipelineModel\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, NaiveBayes, NaiveBayesModel\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = spark.read.parquet(\"sample_a.parquet\")\n",
    "sample_b = spark.read.parquet(\"sample_b.parquet\")\n",
    "sample_c = spark.read.parquet(\"sample_c.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"example\")\n",
    "spark = SparkSession.builder.appName('example').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_a = spark.read.format(\"json\").load('C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\streaming_1a\\\\*')\n",
    "df_a = spark.read.format(\"json\").load('C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\streaming_1a\\\\*')#.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = spark.read.format(\"json\").load('C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\streaming_1b\\\\*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = spark.read.format(\"json\").load('C:\\\\Users\\\\u0115374\\\\Documents\\\\PhD\\\\Courses\\\\Big Data\\\\Assignment3\\\\streaming_1c\\\\*')\n",
    "df_c.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Try to create balanced subsamples of all stream folders'''\n",
    "fractions_a = {'safe': 0.01, 'unsafe': 0.10, 'vandal': 1}\n",
    "fractions_b = {'safe': 0.01, 'unsafe': 0.065, 'vandal': 1}\n",
    "fractions_c = {'safe': 0.012, 'unsafe': 0.065, 'vandal': 1}\n",
    "#sample_a = df_a.sampleBy('label', fractions=fractions_a, seed = 19052020)\n",
    "#sample_b = df_b.sampleBy('label', fractions=fractions_b, seed = 19052020)\n",
    "#sample_c = df_c.sampleBy('label', fractions=fractions_c, seed = 19052020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|unsafe|  288|\n",
      "|  safe|  227|\n",
      "|vandal|  245|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|unsafe|  450|\n",
      "|  safe|  474|\n",
      "|vandal|  416|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe|  584|\n",
      "|unsafe|  636|\n",
      "|vandal|  584|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_a.groupBy(\"label\").count().show()\n",
    "sample_b.groupBy(\"label\").count().show()\n",
    "sample_c.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sample_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_a.write.parquet(\"df_a.parquet\")\n",
    "#sample_a.write.parquet(\"sample_a.parquet\")\n",
    "\n",
    "#df_b.write.parquet(\"df_b.parquet\")\n",
    "#sample_b.write.parquet(\"sample_b.parquet\")\n",
    "\n",
    "#df_c.write.parquet(\"df_c.parquet\")\n",
    "#sample_c.write.parquet(\"sample_c.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diff(old, new):\n",
    "    return '\\n'.join([ l for l in unified_diff(old.split('\\n'), new.split('\\n')) if l.startswith('+') or l.startswith('-') ])\n",
    "\n",
    "udfMake_Diff = udf(make_diff, StringType())\n",
    "\n",
    "def add_diff(df):\n",
    "    df.withColumn(\"diff\", udfMake_Diff(\"text_old\", \"text_new\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffColTransformer(Transformer, ml.util.DefaultParamsWritable, ml.util.DefaultParamsReadable):\n",
    "    '''Custom transformer to get udf makeDiff into pipeline.'''\n",
    "    def __init__(self):\n",
    "        super(DiffColTransformer, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        df = df.withColumn(\"diff\", udfMake_Diff(\"text_old\", \"text_new\"))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = DiffColTransformer()\n",
    "tokenizer = RegexTokenizer(inputCol=\"diff\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "idf = IDF(inputCol=\"vectors\", outputCol=\"features\")\n",
    "label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"target\")\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"target\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [dct, tokenizer, remover, cv, idf, label_indexer, nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(stages=stages).fit(dataset = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(model.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nbmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    '''# Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['nbmodel'] = PipelineModel.load('nbmodel') # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    df_result = globals()['nbmodel'].transform(df)\n",
    "    df_result.select('target').show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
